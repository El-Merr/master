{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 8. Neural Networks\n",
    "\n",
    "**How to train your neurons**\n",
    "\n",
    "Joaquin Vanschoren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: You'll need to install tensorflow-addons\n",
    "#!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preamble import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Global imports and settings\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Using Keras\",tf.keras.__version__)\n",
    "%matplotlib inline\n",
    "HTML('''<style>html, body{overflow-y: visible !important} .CodeMirror{min-width:105% !important;} .rise-enabled .CodeMirror, .rise-enabled .output_subarea{font-size:140%; line-height:1.2; overflow: visible;} .output_subarea pre{width:110%}</style>''') # For slides\n",
    "interactive = True # Set to True for interactive plots \n",
    "if interactive:\n",
    "    plt.rcParams['figure.dpi'] = 150\n",
    "else:\n",
    "    plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "* Neural architectures\n",
    "* Training neural nets\n",
    "    * Forward pass: Tensor operations\n",
    "    * Backward pass: Backpropagation\n",
    "* Neural network design:\n",
    "    * Activation functions\n",
    "    * Weight initialization\n",
    "    * Optimizers\n",
    "* Neural networks in practice\n",
    "* Model selection\n",
    "    * Early stopping\n",
    "    * Memorization capacity and information bottleneck\n",
    "    * L1/L2 regularization\n",
    "    * Dropout\n",
    "    * Batch normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_neural_net(ax, layer_sizes, draw_bias=False, labels=False, activation=False, sigmoid=False,\n",
    "                    weight_count=False, random_weights=False, show_activations=False, figsize=(4, 4)):\n",
    "    \"\"\"\n",
    "    Draws a dense neural net for educational purposes\n",
    "    Parameters:\n",
    "        ax: plot axis\n",
    "        layer_sizes: array with the sizes of every layer\n",
    "        draw_bias: whether to draw bias nodes\n",
    "        labels: whether to draw labels for the weights and nodes\n",
    "        activation: whether to show the activation function inside the nodes\n",
    "        sigmoid: whether the last activation function is a sigmoid\n",
    "        weight_count: whether to show the number of weights and biases\n",
    "        random_weights: whether to show random weights as colored lines\n",
    "        show_activations: whether to show a variable for the node activations\n",
    "        scale_ratio: ratio of the plot dimensions, e.g. 3/4\n",
    "    \"\"\"\n",
    "    left, right, bottom, top = 0.1, 0.89*figsize[0]/figsize[1], 0.1, 0.89\n",
    "    n_layers = len(layer_sizes)\n",
    "    v_spacing = (top - bottom)/float(max(layer_sizes))\n",
    "    h_spacing = (right - left)/float(len(layer_sizes) - 1)\n",
    "    colors = ['greenyellow','cornflowerblue','lightcoral']\n",
    "    w_count, b_count = 0, 0\n",
    "    ax.set_xlim(0, figsize[0]/figsize[1])\n",
    "    ax.axis('off')\n",
    "    ax.set_aspect('equal')\n",
    "    txtargs = {\"fontsize\":12, \"verticalalignment\":'center', \"horizontalalignment\":'center', \"zorder\":5}\n",
    "    \n",
    "    # Draw biases by adding a node to every layer except the last one\n",
    "    if draw_bias:\n",
    "        layer_sizes = [x+1 for x in layer_sizes]\n",
    "        layer_sizes[-1] = layer_sizes[-1] - 1\n",
    "        \n",
    "    # Nodes\n",
    "    for n, layer_size in enumerate(layer_sizes):\n",
    "        layer_top = v_spacing*(layer_size - 1)/2. + (top + bottom)/2. \n",
    "        node_size = v_spacing/len(layer_sizes) if activation and n!=0 else v_spacing/3.\n",
    "        if n==0:\n",
    "            color = colors[0]\n",
    "        elif n==len(layer_sizes)-1:\n",
    "            color = colors[2]\n",
    "        else:\n",
    "            color = colors[1]\n",
    "        for m in range(layer_size):\n",
    "            ax.add_artist(plt.Circle((n*h_spacing + left, layer_top - m*v_spacing), radius=node_size,\n",
    "                                      color=color, ec='k', zorder=4))\n",
    "            b_count += 1\n",
    "            nx, ny = n*h_spacing + left, layer_top - m*v_spacing\n",
    "            nsx, nsy = [n*h_spacing + left,n*h_spacing + left], [layer_top - m*v_spacing - 0.5*node_size*2,layer_top - m*v_spacing + 0.5*node_size*2]\n",
    "            if draw_bias and m==0 and n<len(layer_sizes)-1:\n",
    "                ax.text(nx, ny, r'$1$', **txtargs)\n",
    "            elif labels and n==0:\n",
    "                ax.text(n*h_spacing + left,layer_top + v_spacing/1.5, 'input', **txtargs)\n",
    "                ax.text(nx, ny, r'$x_{}$'.format(m), **txtargs)\n",
    "            elif labels and n==len(layer_sizes)-1:\n",
    "                if activation:\n",
    "                    if sigmoid:\n",
    "                        ax.text(n*h_spacing + left,layer_top - m*v_spacing, r\"$z \\;\\;\\; \\sigma$\", **txtargs)\n",
    "                    else:\n",
    "                        ax.text(n*h_spacing + left,layer_top - m*v_spacing, r\"$z_{} \\;\\; g$\".format(m), **txtargs)\n",
    "                    ax.add_artist(plt.Line2D(nsx, nsy, c='k', zorder=6))\n",
    "                    if show_activations:        \n",
    "                        ax.text(n*h_spacing + left + 1.5*node_size,layer_top - m*v_spacing, r\"$\\hat{y}$\", fontsize=12, \n",
    "                                verticalalignment='center', horizontalalignment='left', zorder=5, c='r')\n",
    "\n",
    "                else:\n",
    "                    ax.text(nx, ny, r'$o_{}$'.format(m), **txtargs)\n",
    "                ax.text(n*h_spacing + left,layer_top + v_spacing, 'output', **txtargs)\n",
    "            elif labels:\n",
    "                if activation:\n",
    "                    ax.text(n*h_spacing + left,layer_top - m*v_spacing, r\"$z_{} \\;\\; f$\".format(m), **txtargs)\n",
    "                    ax.add_artist(plt.Line2D(nsx, nsy, c='k', zorder=6))\n",
    "                    if show_activations:        \n",
    "                        ax.text(n*h_spacing + left + node_size,layer_top - m*v_spacing, r\"$a_{}$\".format(m), fontsize=12, \n",
    "                                verticalalignment='center', horizontalalignment='left', zorder=5, c='b')\n",
    "                else:\n",
    "                    ax.text(nx, ny, r'$h_{}$'.format(m), **txtargs)\n",
    "                \n",
    "            \n",
    "    # Edges\n",
    "    for n, (layer_size_a, layer_size_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "        layer_top_a = v_spacing*(layer_size_a - 1)/2. + (top + bottom)/2.\n",
    "        layer_top_b = v_spacing*(layer_size_b - 1)/2. + (top + bottom)/2.\n",
    "        for m in range(layer_size_a):\n",
    "            for o in range(layer_size_b):\n",
    "                if not (draw_bias and o==0 and len(layer_sizes)>2 and n<layer_size_b-1):\n",
    "                    xs = [n*h_spacing + left, (n + 1)*h_spacing + left]\n",
    "                    ys = [layer_top_a - m*v_spacing, layer_top_b - o*v_spacing]\n",
    "                    color = 'k' if not random_weights else plt.cm.bwr(np.random.random())\n",
    "                    ax.add_artist(plt.Line2D(xs, ys, c=color, lw=1, alpha=0.6))\n",
    "                    if not (draw_bias and m==0):\n",
    "                        w_count += 1\n",
    "                    if labels and not random_weights:\n",
    "                        wl = r'$w_{{{},{}}}$'.format(m,o) if layer_size_b>1 else r'$w_{}$'.format(m)\n",
    "                        ax.text(xs[0]+np.diff(xs)/2, np.mean(ys)-np.diff(ys)/9, wl, ha='center', va='center', \n",
    "                                 fontsize=10)\n",
    "    # Count\n",
    "    if weight_count:\n",
    "        b_count = b_count - layer_sizes[0]\n",
    "        if draw_bias:\n",
    "            b_count = b_count - (len(layer_sizes) - 2)\n",
    "        ax.text(right, bottom, \"{} weights, {} biases\".format(w_count, b_count), ha='center', va='center')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear models as a building block\n",
    "* Logistic regression, drawn in a different, neuro-inspired, way\n",
    "    * Linear model: inner product ($z$) of input vector $\\mathbf{x}$ and weight vector $\\mathbf{w}$, plus bias $w_0$\n",
    "    * Logistic (or sigmoid) function maps the output to a probability in [0,1]\n",
    "    * Uses log loss (cross-entropy) and gradient descent to learn the weights\n",
    "        \n",
    "$$\\hat{y}(\\mathbf{x}) = \\text{sigmoid}(z) = \\text{sigmoid}(w_0 + \\mathbf{w}\\mathbf{x}) = \\text{sigmoid}(w_0 + w_1 * x_1 + w_2 * x_2 +... + w_p * x_p)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(3, 3))\n",
    "ax = fig.gca()\n",
    "draw_neural_net(ax, [4, 1], activation=True, draw_bias=True, labels=True, sigmoid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Basic Architecture\n",
    "* Add one (or more) _hidden_ layers $h$ with $k$ nodes (or units, cells, neurons)\n",
    "    * Every 'neuron' is a tiny function, the network is an arbitrarily complex function\n",
    "    * Weights $w_{i,j}$ between node $i$ and node $j$ form a weight matrix $\\mathbf{W}^{(l)}$ per layer $l$\n",
    "* Every neuron weights the inputs $\\mathbf{x}$ and passes it through a non-linear activation function\n",
    "    * Activation functions ($f,g$) can be different per layer, output $\\mathbf{a}$ is called activation\n",
    "$$\\color{blue}{h(\\mathbf{x})} = \\color{blue}{\\mathbf{a}} = f(\\mathbf{z}) = f(\\mathbf{W}^{(1)} \\color{green}{\\mathbf{x}}+\\mathbf{w}^{(1)}_0) \\quad \\quad \\color{red}{o(\\mathbf{x})} = g(\\mathbf{W}^{(2)}  \\color{blue}{\\mathbf{a}}+\\mathbf{w}^{(2)}_0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(8, 4))\n",
    "draw_neural_net(axes[0], [2, 3, 1],  draw_bias=True, labels=True, weight_count=True)\n",
    "draw_neural_net(axes[1], [2, 3, 1],  activation=True, show_activations=True, draw_bias=True, labels=True, weight_count=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More layers\n",
    "* Add more layers, and more nodes per layer, to make the model more complex\n",
    "    * For simplicity, we don't draw the biases (but remember that they are there)\n",
    "* In _dense_ (fully-connected) layers, every previous layer node is connected to all nodes\n",
    "* The output layer can also have multiple nodes (e.g. 1 per class in multi-class classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "@interact\n",
    "def plot_dense_net(nr_layers=(0,6,1), nr_nodes=(1,12,1)):\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    ax = fig.gca()\n",
    "    ax.axis('off')\n",
    "    hidden = [nr_nodes]*nr_layers\n",
    "    draw_neural_net(ax, [5] + hidden + [5], weight_count=True, figsize=(6, 4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    plot_dense_net(nr_layers=6, nr_nodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why layers?\n",
    "* Each layer acts as a _filter_ and learns a new _representation_ of the data\n",
    "    * Subsequent layers can learn iterative refinements\n",
    "    * Easier that learning a complex relationship in one go\n",
    "* Example: for image input, each layer yields new (filtered) images\n",
    "    * Can learn multiple mappings at once: weight _tensor_ $\\mathit{W}$ yields activation tensor $\\mathit{A}$\n",
    "    * From low-level patterns (edges, end-points, ...) to combinations thereof\n",
    "    * Each neuron 'lights up' if certain patterns occur in the input\n",
    "\n",
    "<img src=\"../images/00_layers2.png\" alt=\"ml\" style=\"width: 50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other architectures\n",
    "* There exist MANY types of networks for many different tasks\n",
    "* Convolutional nets for image data, Recurrent nets for sequential data,...\n",
    "* Also used to learn representations (embeddings), generate new images, text,...\n",
    "\n",
    "<img src=\"../images/neural_zoo.png\" alt=\"ml\" style=\"width: 1200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Neural Nets\n",
    "* Design the architecture, choose activation functions (e.g. sigmoids)\n",
    "* Choose a way to initialize the weights (e.g. random initialization)\n",
    "* Choose a _loss function_ (e.g. log loss) to measure how well the model fits training data\n",
    "* Choose an _optimizer_ (typically an SGD variant) to update the weights\n",
    "\n",
    "<img src=\"../images/09_overview.png\" alt=\"ml\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mini-batch Stochastic Gradient Descent (recap)\n",
    "1. Draw a batch of *batch_size* training data $\\mathbf{X}$ and $\\mathbf{y}$\n",
    "2. _Forward pass_ : pass $\\mathbf{X}$ though the network to yield predictions $\\mathbf{\\hat{y}}$\n",
    "3. Compute the loss $\\mathcal{L}$ (mismatch between  $\\mathbf{\\hat{y}}$ and $\\mathbf{y}$)\n",
    "4. _Backward pass_ : Compute the gradient of the loss with regard to every weight\n",
    "    * _Backpropagate_ the gradients through all the layers\n",
    "5. Update $W$: $W_{(i+1)} = W_{(i)} - \\frac{\\partial L(x, W_{(i)})}{\\partial W} * \\eta$\n",
    "\n",
    "Repeat until n passes (epochs) are made through the entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: show the actual weight updates\n",
    "@interact\n",
    "def draw_updates(iteration=(1,100,1)):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "    np.random.seed(iteration)\n",
    "    draw_neural_net(ax, [6,5,5,3], labels=True, random_weights=True, show_activations=True, figsize=(6, 4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    draw_updates(iteration=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forward pass\n",
    "* We can naturally represent the data as _tensors_\n",
    "    * Numerical n-dimensional array (with n axes)\n",
    "    * 2D tensor: matrix (samples, features)\n",
    "    * 3D tensor: time series (samples, timesteps, features)\n",
    "    * 4D tensor: color images (samples, height, width, channels)\n",
    "    * 5D tensor: video (samples, frames, height, width, channels)  \n",
    "    \n",
    "<img src=\"../images/08_timeseries.png\" alt=\"ml\" style=\"float: left; width: 30%;\"/>\n",
    "<img src=\"../images/08_images.png\" alt=\"ml\" style=\"float: left; width: 30%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Tensor operations\n",
    "* The operations that the network performs on the data can be reduced to a _series of tensor operations_\n",
    "    * These are also much easier to run on GPUs\n",
    "* A dense layer with sigmoid activation, input tensor $\\mathbf{X}$, weight tensor $\\mathbf{W}$, bias $\\mathbf{b}$:\n",
    "\n",
    "``` python\n",
    "y = sigmoid(np.dot(X, W) + b)\n",
    "```\n",
    "* Tensor dot product for 2D inputs ($a$ samples, $b$ features, $c$ hidden nodes)\n",
    "\n",
    "<img src=\"../images/08_dot.png\" alt=\"ml\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Element-wise operations\n",
    "\n",
    "* Activation functions and addition are element-wise operations:\n",
    "\n",
    "``` python\n",
    "def sigmoid(x):\n",
    "  return 1/(1 + np.exp(-x)) \n",
    "\n",
    "def add(x, y):\n",
    "  return x + y\n",
    "```\n",
    "\n",
    "* Note: if y has a lower dimension than x, it will be _broadcasted_: axes are added to match the dimensionality, and y is repeated along the new axes \n",
    "\n",
    "``` python\n",
    ">>> np.array([[1,2],[3,4]]) + np.array([10,20])\n",
    "array([[11, 22],\n",
    "       [13, 24]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backward pass (backpropagation)\n",
    "* For last layer, compute gradient of the loss function $\\mathcal{L}$ w.r.t all weights of layer $l$\n",
    "\n",
    "$$\\nabla \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}} = \n",
    "                  \\begin{bmatrix}\n",
    "                    \\frac{\\partial \\mathcal{L}}{\\partial w_{0,0}} & \\ldots & \\frac{\\partial \\mathcal{L}}{\\partial w_{0,l}} \\\\\n",
    "                    \\vdots & \\ddots & \\vdots \\\\\n",
    "                    \\frac{\\partial \\mathcal{L}}{\\partial w_{k,0}}  & \\ldots & \\frac{\\partial \\mathcal{L}}{\\partial w_{k,l}}\n",
    "                  \\end{bmatrix} \\\\[15pt]$$\n",
    "                  \n",
    "* Sum up the gradients for all $\\mathbf{x}_j$ in minibatch: $\\sum_{j} \\frac{\\partial \\mathcal{L}(\\mathbf{x}_j,y_j)}{\\partial W^{(l)}}$\n",
    "* Update all weights in a layer at once (with learning rate $\\eta$): $W_{(i+1)}^{(l)} = W_{(i)}^{(l)} - \\eta \\sum_{j} \\frac{\\partial \\mathcal{L}(\\mathbf{x}_j,y_j)}{\\partial W_{(i)}^{(l)}}$ \n",
    "* Repeat for next layer, iterating backwards (most efficient, avoids redundant calculations)\n",
    "    \n",
    "<img src=\"../images/01_gradient_descent.jpg\" alt=\"ml\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backpropagation (example)\n",
    "* Imagine feeding a single data point, output is $\\hat{y} = g(z) = g(w_0 + w_1 * a_1 + w_2 * a_2 +... + w_p * a_p)$\n",
    "* Decrease loss by updating weights:\n",
    "    * Update the weights of last layer to maximize improvement: \n",
    "   $w_{i,(new)} = w_{i} - \\frac{\\partial \\mathcal{L}}{\\partial w_i} * \\eta$\n",
    "    * To compute gradient $\\frac{\\partial \\mathcal{L}}{\\partial w_i}$ we need the chain rule: $f(g(x)) = f'(g(x)) * g'(x)$\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial w_i} = \\color{red}{\\frac{\\partial \\mathcal{L}}{\\partial g}} \\color{blue}{\\frac{\\partial \\mathcal{g}}{\\partial z_0}} \\color{green}{\\frac{\\partial \\mathcal{z_0}}{\\partial w_i}}$$\n",
    "* E.g., with $\\mathcal{L} = \\frac{1}{2}(y-\\hat{y})^2$ and sigmoid $\\sigma$: $\\frac{\\partial \\mathcal{L}}{\\partial w_i} = \\color{red}{(y - \\hat{y})} * \\color{blue}{\\sigma'(z_0)} * \\color{green}{a_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 3.5))\n",
    "ax = fig.gca()\n",
    "draw_neural_net(ax, [2, 3, 1],  activation=True, draw_bias=True, labels=True, \n",
    "                show_activations=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backpropagation (2)\n",
    "* Another way to decrease the loss $\\mathcal{L}$ is to update the activations $a_i$\n",
    "    * To update $a_i = f(z_i)$, we need to update the weights of the previous layer\n",
    "    * We want to nudge $a_i$ in the right direction by updating $w_{i,j}$:\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial w_{i,j}} = \\frac{\\partial \\mathcal{L}}{\\partial a_i} \\frac{\\partial a_i}{\\partial z_i} \\frac{\\partial \\mathcal{z_i}}{\\partial w_{i,j}} = \\left( \\frac{\\partial \\mathcal{L}}{\\partial g} \\frac{\\partial \\mathcal{g}}{\\partial z_0} \\frac{\\partial \\mathcal{z_0}}{\\partial a_i} \\right) \\frac{\\partial a_i}{\\partial z_i} \\frac{\\partial \\mathcal{z_i}}{\\partial w_{i,j}}$$\n",
    "    * We know $\\frac{\\partial \\mathcal{L}}{\\partial g}$ and $\\frac{\\partial \\mathcal{g}}{\\partial z_0}$ from the previous step, $\\frac{\\partial \\mathcal{z_0}}{\\partial a_i} = w_i$, $\\frac{\\partial a_i}{\\partial z_i} = f'$ and $\\frac{\\partial \\mathcal{z_i}}{\\partial w_{i,j}} = x_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.gca()\n",
    "draw_neural_net(ax, [2, 3, 1],  activation=True, draw_bias=True, labels=True, \n",
    "                show_activations=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backpropagation (3)\n",
    "* With multiple output nodes, $\\mathcal{L}$ is the sum of all per-output (per-class) losses\n",
    "    * $\\frac{\\partial \\mathcal{L}}{\\partial a_i}$ is sum of the gradients for every output\n",
    "* Per layer, sum up gradients for every point $\\mathbf{x}$ in the batch: $\\sum_{j} \\frac{\\partial \\mathcal{L}(\\mathbf{x}_j,y_j)}{\\partial W}$\n",
    "* Update all weights of every layer $l$\n",
    "    * $W_{(i+1)}^{(l)} = W_{(i)}^{(l)} - \\eta \\sum_{j} \\frac{\\partial \\mathcal{L}(\\mathbf{x}_j,y_j)}{\\partial W_{(i)}^{(l)}}$ \n",
    "* Repeat with a new batch of data until loss converges\n",
    "* [Nice animation of the entire process](https://youtu.be/Ilg3gGewQ5U?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=403)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax = fig.gca()\n",
    "draw_neural_net(ax, [2, 3, 3, 2],  activation=True, draw_bias=True, labels=True, \n",
    "                random_weights=True, show_activations=True, figsize=(8, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backpropagation (summary)\n",
    "* The network output $a_o$ is defined by the weights $W^{(o)}$ and biases $\\mathbf{b}^{(o)}$ of the output layer, and\n",
    "* The activations of a hidden layer $h_1$ with activation function $a_{h_1}$, weights $W^{(1)}$ and biases $\\mathbf{b^{(1)}}$:\n",
    "\n",
    "$$\\color{red}{a_o(\\mathbf{x})} = \\color{red}{a_o(\\mathbf{z_0})} = \\color{red}{a_o(W^{(o)}} \\color{blue}{a_{h_1}(z_{h_1})} \\color{red}{+ \\mathbf{b}^{(o)})} = \\color{red}{a_o(W^{(o)}} \\color{blue}{a_{h_1}(W^{(1)} \\color{green}{\\mathbf{x}} + \\mathbf{b}^{(1)})}\n",
    "  \\color{red}{+ \\mathbf{b}^{(o)})} $$\n",
    "  \n",
    "* Minimize the loss by SGD. For layer $l$, compute $\\frac{\\partial \\mathcal{L}(a_o(x))}{\\partial W_l}$ and $\\frac{\\partial \\mathcal{L}(a_o(x))}{\\partial b_{l,i}}$ using the chain rule\n",
    "* Decomposes into <span style=\"color:red\">gradient of layer above</span>, <span style=\"color:blue\">gradient of activation function</span>, <span style=\"color:green\">gradient of layer input</span>:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}(a_o)}{\\partial W^{(1)}} = \\color{red}{\\frac{\\partial \\mathcal{L}(a_o)}{\\partial a_{h_1}}} \\color{blue}{\\frac{\\partial a_{h_1}}{\\partial z_{h_1}}} \\color{green}{\\frac{\\partial z_{h_1}}{\\partial W^{(1)}}} \n",
    "= \\left( \\color{red}{\\frac{\\partial \\mathcal{L}(a_o)}{\\partial a_o}} \\color{blue}{\\frac{\\partial a_o}{\\partial z_o}} \\color{green}{\\frac{\\partial z_o}{\\partial a_{h_1}}}\\right) \\color{blue}{\\frac{\\partial a_{h_1}}{\\partial z_{h_1}}} \\color{green}{\\frac{\\partial z_{h_1}}{\\partial W^{(1)}}}  $$\n",
    "\n",
    "<img src=\"../images/backprop_schema2.png\" alt=\"ml\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activation functions for hidden layers\n",
    "* Sigmoid: $f(z) = \\frac{1}{1+e^{-z}}$\n",
    "* Tanh: $f(z) = \\frac{2}{1+e^{-2z}} - 1$ \n",
    "    * Activations around 0 are better for gradient descent convergence\n",
    "* Rectified Linear (ReLU): $f(z) = max(0,z)$ \n",
    "    * Less smooth, but much faster (note: not differentiable at 0)\n",
    "* Leaky ReLU: $f(z) = \\begin{cases} 0.01z & z<0 \\\\ z & otherwise \\end{cases}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(X, function=\"sigmoid\"):     \n",
    "    if function == \"sigmoid\":      \n",
    "        return 1.0/(1.0 + np.exp(-X))    \n",
    "    if function == \"softmax\": \n",
    "        return np.exp(X) / np.sum(np.exp(X), axis=0)   \n",
    "    elif function == \"tanh\":      \n",
    "        return np.tanh(X)    \n",
    "    elif function == \"relu\":      \n",
    "        return np.maximum(0,X)    \n",
    "    elif function == \"leaky_relu\":      \n",
    "        return np.maximum(0.1*X,X)\n",
    "    elif function == \"none\":      \n",
    "        return X\n",
    "    \n",
    "def activation_derivative(X, function=\"sigmoid\"):   \n",
    "    if function == \"sigmoid\": \n",
    "        sig = 1.0/(1.0 + np.exp(-X))   \n",
    "        return sig * (1 - sig)\n",
    "    elif function == \"tanh\":      \n",
    "        return 1 - np.tanh(X)**2   \n",
    "    elif function == \"relu\":      \n",
    "        return np.where(X > 0, 1, 0)\n",
    "    elif function == \"leaky_relu\":    \n",
    "        # Using 0.1 instead of 0.01 to make it visible in the plot\n",
    "        return np.where(X > 0, 1, 0.1)\n",
    "    elif function == \"none\":      \n",
    "        return X/X\n",
    "    \n",
    "def plot_activation(function, ax, derivative=False):\n",
    "    if function==\"softmax\":       \n",
    "        x = np.linspace(-6,6,9)\n",
    "        ax.plot(x,activation(x, function),lw=2, c='b', linestyle='-', marker='o')\n",
    "    else:     \n",
    "        x = np.linspace(-6,6,101)\n",
    "        ax.plot(x,activation(x, function),lw=2, c='b', linestyle='-') \n",
    "        if derivative:\n",
    "            if function == \"relu\" or function == \"leaky_relu\":\n",
    "                ax.step(x,activation_derivative(x, function),lw=2, c='r', linestyle='-')\n",
    "            else:\n",
    "                ax.plot(x,activation_derivative(x, function),lw=2, c='r', linestyle='-')\n",
    "    ax.set_xlabel(\"input\")\n",
    "    ax.set_ylabel(function)\n",
    "    ax.grid()\n",
    "    \n",
    "functions = [\"sigmoid\",\"tanh\",\"relu\",\"leaky_relu\"]\n",
    "\n",
    "@interact\n",
    "def plot_activations(function=functions):\n",
    "    fig, ax = plt.subplots(figsize=(6,2))\n",
    "    plot_activation(function, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    fig, axes = plt.subplots(1,4, figsize=(10,2))\n",
    "    for function, ax in zip(functions,axes):\n",
    "        plot_activation(function, ax)\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Effect of activation functions on the gradient\n",
    "* During gradient descent, the gradient depends on the activation function $a_{h}$: $\\frac{\\partial \\mathcal{L}(a_o)}{\\partial W^{(l)}} = \\color{red}{\\frac{\\partial \\mathcal{L}(a_o)}{\\partial a_{h_l}}} \\color{blue}{\\frac{\\partial a_{h_l}}{\\partial z_{h_l}}} \\color{green}{\\frac{\\partial z_{h_l}}{\\partial W^{(l)}}}$\n",
    "* If derivative of the activation function $\\color{blue}{\\frac{\\partial a_{h_l}}{\\partial z_{h_l}}}$ is 0, the weights $w_i$ are not updated\n",
    "    * Moreover, the gradients of previous layers will be reduced (vanishing gradient)\n",
    "* sigmoid, tanh: gradient is very small for large inputs: slow updates\n",
    "* With ReLU, $\\color{blue}{\\frac{\\partial a_{h_l}}{\\partial z_{h_l}}} = 1$ if $z>0$, hence better against vanishing gradients\n",
    "    * Problem: for very negative inputs, the gradient is 0 and may never recover (dying ReLU)\n",
    "    * Leaky ReLU has a small (0.01) gradient there to allow recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def plot_activations_derivative(function=functions):\n",
    "    fig, ax = plt.subplots(figsize=(6,2))\n",
    "    plot_activation(function, ax, derivative=True)\n",
    "    plt.legend(['original','derivative'], loc='upper center', \n",
    "               bbox_to_anchor=(0.5, 1.25), ncol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    fig, axes = plt.subplots(1,4, figsize=(10,2))\n",
    "    for function, ax in zip(functions,axes):\n",
    "        plot_activation(function, ax, derivative=True)\n",
    "    fig.legend(['original','derivative'], loc='upper center', \n",
    "               bbox_to_anchor=(0.5, 1.25), ncol=2)\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### ReLU vs Tanh\n",
    "* What is the effect of using non-smooth activation functions?\n",
    "    * ReLU produces piecewise-linear boundaries, but allows deeper networks\n",
    "    * Tanh produces smoother decision boundaries, but is slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mglearn.plot_2d_separator import plot_2d_classification\n",
    "import time\n",
    "\n",
    "@interact\n",
    "def plot_boundary(nr_layers=(1,4,1)):\n",
    "    X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                        random_state=42)\n",
    "    \n",
    "    # Multi-Layer Perceptron with ReLU\n",
    "    mlp = MLPClassifier(solver='lbfgs', random_state=0,\n",
    "                        hidden_layer_sizes=[10]*nr_layers)\n",
    "    start = time.time()\n",
    "    mlp.fit(X_train, y_train)\n",
    "    relu_time = time.time() - start\n",
    "    relu_acc = mlp.score(X_test, y_test)\n",
    "\n",
    "    # Multi-Layer Perceptron with tanh\n",
    "    mlp_tanh = MLPClassifier(solver='lbfgs', activation='tanh',\n",
    "                             random_state=0, hidden_layer_sizes=[10]*nr_layers)\n",
    "    start = time.time()\n",
    "    mlp_tanh.fit(X_train, y_train)\n",
    "    tanh_time = time.time() - start\n",
    "    tanh_acc = mlp_tanh.score(X_test, y_test)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    axes[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='bwr', label=\"train\")\n",
    "    axes[0].set_title(\"ReLU, acc: {:.2f}, time: {:.2f} sec\".format(relu_acc, relu_time))\n",
    "    plot_2d_classification(mlp, X_train, fill=True, cm='bwr', alpha=.3, ax=axes[0])\n",
    "    axes[1].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='bwr', label=\"train\")\n",
    "    axes[1].set_title(\"tanh, acc: {:.2f}, time: {:.2f} sec\".format(tanh_acc, tanh_time))\n",
    "    plot_2d_classification(mlp_tanh, X_train, fill=True, cm='bwr', alpha=.3, ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    plot_boundary(nr_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Activation functions for output layer\n",
    "* _sigmoid_ converts output to probability in [0,1]\n",
    "    * For binary classification \n",
    "* _softmax_ converts all outputs (aka 'logits') to probabilities that sum up to 1\n",
    "    * For multi-class classification ($k$ classes)\n",
    "    * Can cause over-confident models. If so, smooth the labels: $y_{smooth} = (1-\\alpha)y + \\frac{\\alpha}{k}$\n",
    "$$\\text{softmax}(\\mathbf{x},i) = \\frac{e^{x_i}}{\\sum_{j=1}^k e^{x_j}}$$\n",
    "* For regression, don't use any activation function, let the model learn the exact target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_functions = [\"sigmoid\",\"softmax\",\"none\"]\n",
    "\n",
    "@interact\n",
    "def plot_output_activation(function=output_functions):\n",
    "    fig, ax = plt.subplots(figsize=(6,2))\n",
    "    plot_activation(function, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    fig, axes = plt.subplots(1,2, figsize=(8,2))\n",
    "    for function, ax in zip(output_functions[:2],axes):\n",
    "        plot_activation(function, ax)\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Weight initialization\n",
    "* Initializing weights to 0 is bad: all gradients in layer will be identical (symmetry)\n",
    "* Too small random weights shrink activations to 0 along the layers (vanishing gradient)\n",
    "* Too large random weights multiply along layers (exploding gradient, zig-zagging)\n",
    "* Ideal: small random weights + variance of input and output gradients remains the same\n",
    "  * Glorot/Xavier initialization (for tanh): randomly sample from  $N(0,\\sigma), \\sigma = \\sqrt{\\frac{2}{\\text{fan_in + fan_out}}}$\n",
    "    * fan_in: number of input units, fan_out: number of output units\n",
    "  * He initialization (for ReLU): randomly sample from  $N(0,\\sigma), \\sigma = \\sqrt{\\frac{2}{\\text{fan_in}}}$\n",
    "  * Uniform sampling (instead of $N(0,\\sigma)$) for deeper networks (w.r.t. vanishing gradients)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(6, 3))\n",
    "draw_neural_net(ax, [3, 5, 5, 5, 5, 5, 3], random_weights=True, figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weight initialization: transfer learning\n",
    "* Instead of starting from scratch, start from weights previously learned from similar tasks\n",
    "    * This is, to a big extent, how humans learn so fast\n",
    "* Transfer learning: learn weights on task T, transfer them to new network\n",
    "    * Weights can be frozen, or finetuned to the new data\n",
    "* Only works if the previous task is 'similar' enough\n",
    "    * Meta-learning: learn a good initialization across many related tasks\n",
    "\n",
    "<img src=\"../images/transfer_learning.png\" alt=\"ml\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code adapted from Il Gu Yi: https://github.com/ilguyi/optimizers.numpy\n",
    "from matplotlib.colors import LogNorm\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# Toy surface\n",
    "def f(x, y):\n",
    "    return (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2\n",
    "\n",
    "# Tensorflow optimizers\n",
    "sgd = tf.optimizers.SGD(0.01)\n",
    "lr_schedule = tf.optimizers.schedules.ExponentialDecay(0.02,decay_steps=100,decay_rate=0.96)\n",
    "sgd_decay = tf.optimizers.SGD(learning_rate=lr_schedule)\n",
    "#sgd_cyclic = tfa.optimizers.CyclicalLearningRate(initial_learning_rate= 0.1, \n",
    "#maximal_learning_rate= 0.5, step_size=0.05)\n",
    "clr_schedule = tfa.optimizers.CyclicalLearningRate(initial_learning_rate=1e-4, maximal_learning_rate= 0.1, \n",
    "                                                   step_size=100, scale_fn=lambda x : x)\n",
    "sgd_cyclic = tf.optimizers.SGD(learning_rate=clr_schedule)\n",
    "momentum = tf.optimizers.SGD(0.005, momentum=0.9, nesterov=False)\n",
    "nesterov = tf.optimizers.SGD(0.005, momentum=0.9, nesterov=True)\n",
    "adagrad = tf.optimizers.Adagrad(0.4)\n",
    "adamax = tf.optimizers.Adamax(learning_rate=0.5, beta_1=0.9, beta_2=0.999)\n",
    "#adadelta = tf.optimizers.Adadelta(learning_rate=1.0)\n",
    "rmsprop = tf.optimizers.RMSprop(learning_rate=0.1)\n",
    "rmsprop_momentum = tf.optimizers.RMSprop(learning_rate=0.1, momentum=0.9)\n",
    "adam = tf.optimizers.Adam(learning_rate=0.2, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "\n",
    "optimizers = [sgd, sgd_decay, sgd_cyclic, momentum, nesterov, adagrad, rmsprop,  rmsprop_momentum, adam, adamax]\n",
    "opt_names = ['sgd', 'sgd_decay', 'sgd_cyclic', 'momentum', 'nesterov', 'adagrad', 'rmsprop', 'rmsprop_mom', 'adam', 'adamax']\n",
    "cmap = plt.cm.get_cmap('tab10')\n",
    "colors = [cmap(x/10) for x in range(10)]\n",
    "\n",
    "# Training\n",
    "all_paths = []\n",
    "for opt, name in zip(optimizers, opt_names):\n",
    "    x_init = 0.8\n",
    "    x = tf.compat.v1.get_variable('x', dtype=tf.float32, initializer=tf.constant(x_init))\n",
    "    y_init = 1.6\n",
    "    y = tf.compat.v1.get_variable('y', dtype=tf.float32, initializer=tf.constant(y_init))\n",
    "\n",
    "    x_history = []\n",
    "    y_history = []\n",
    "    z_prev = 0.0\n",
    "    max_steps = 100\n",
    "    for step in range(max_steps):\n",
    "        with tf.GradientTape() as g:\n",
    "            z = f(x, y)\n",
    "            x_history.append(x.numpy())\n",
    "            y_history.append(y.numpy())\n",
    "            dz_dx, dz_dy = g.gradient(z, [x, y])\n",
    "            opt.apply_gradients(zip([dz_dx, dz_dy], [x, y]))\n",
    "\n",
    "    if np.abs(z_prev - z.numpy()) < 1e-6:\n",
    "        break\n",
    "    z_prev = z.numpy()\n",
    "    x_history = np.array(x_history)\n",
    "    y_history = np.array(y_history)\n",
    "    path = np.concatenate((np.expand_dims(x_history, 1), np.expand_dims(y_history, 1)), axis=1).T\n",
    "    all_paths.append(path)\n",
    "        \n",
    "# Plotting\n",
    "number_of_points = 50\n",
    "margin = 4.5\n",
    "minima = np.array([3., .5])\n",
    "minima_ = minima.reshape(-1, 1)\n",
    "x_min = 0. - 2\n",
    "x_max = 0. + 3.5\n",
    "y_min = 0. - 3.5\n",
    "y_max = 0. + 2\n",
    "x_points = np.linspace(x_min, x_max, number_of_points) \n",
    "y_points = np.linspace(y_min, y_max, number_of_points)\n",
    "x_mesh, y_mesh = np.meshgrid(x_points, y_points)\n",
    "z = np.array([f(xps, yps) for xps, yps in zip(x_mesh, y_mesh)])\n",
    "\n",
    "def plot_optimizers(ax, iterations, optimizers):\n",
    "    ax.contour(x_mesh, y_mesh, z, levels=np.logspace(-0.5, 5, 25), norm=LogNorm(), cmap=plt.cm.jet)\n",
    "    ax.plot(*minima, 'r*', markersize=20)\n",
    "    for name, path, color in zip(opt_names, all_paths, colors):\n",
    "        if name in optimizers:\n",
    "            p = path[:,:iterations]\n",
    "            ax.quiver(p[0,:-1], p[1,:-1], p[0,1:]-p[0,:-1], p[1,1:]-p[1,:-1], scale_units='xy', angles='xy', scale=1, color=color, lw=3)\n",
    "            ax.plot([], [], color=color, label=name, lw=3, linestyle='-')\n",
    "\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_xlim((x_min, x_max))\n",
    "    ax.set_ylim((y_min, y_max))\n",
    "    ax.legend(loc='lower left', prop={'size': 15})\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy plot to illustrate nesterov momentum\n",
    "# TODO: replace with actual gradient computation?\n",
    "def plot_nesterov(ax, method=\"Nesterov momentum\"):\n",
    "    ax.contour(x_mesh, y_mesh, z, levels=np.logspace(-0.5, 5, 25), norm=LogNorm(), cmap=plt.cm.jet)\n",
    "    ax.plot(*minima, 'r*', markersize=20)\n",
    "    \n",
    "    # toy example\n",
    "    ax.quiver(-0.8,-1.13,1,1.33, scale_units='xy', angles='xy', scale=1, color='k', alpha=0.5, lw=3, label=\"previous update\")\n",
    "    # 0.9 * previous update\n",
    "    ax.quiver(0.2,0.2,0.9,1.2, scale_units='xy', angles='xy', scale=1, color='g', lw=3, label=\"momentum step\")\n",
    "    if method == \"Momentum\":\n",
    "        ax.quiver(0.2,0.2,0.5,0, scale_units='xy', angles='xy', scale=1, color='r', lw=3, label=\"gradient step\")\n",
    "        ax.quiver(0.2,0.2,0.9*0.9+0.5,1.2, scale_units='xy', angles='xy', scale=1, color='b', lw=3, label=\"actual step\")\n",
    "    if method == \"Nesterov momentum\":\n",
    "        ax.quiver(1.1,1.4,-0.2,-1, scale_units='xy', angles='xy', scale=1, color='r', lw=3, label=\"'lookahead' gradient step\")\n",
    "        ax.quiver(0.2,0.2,0.7,0.2, scale_units='xy', angles='xy', scale=1, color='b', lw=3, label=\"actual step\")\n",
    "\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(method)\n",
    "    ax.set_xlim((x_min, x_max))\n",
    "    ax.set_ylim((-2.5, y_max))\n",
    "    ax.legend(loc='lower right', prop={'size': 9})\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimizers\n",
    "### SGD with learning rate schedules\n",
    "* Using a constant learning $\\eta$ rate for weight updates $\\mathbf{w}_{(s+1)} = \\mathbf{w}_s-\\eta\\nabla \\mathcal{L}(\\mathbf{w}_s)$ is not ideal\n",
    "* Learning rate decay/annealing with decay rate $k$\n",
    "    * E.g. exponential ($\\eta_{s+1} = \\eta_{s}  e^{-ks}$), inverse-time ($\\eta_{s+1} = \\frac{\\eta_{0}}{1+ks}$),...\n",
    "* Cyclical learning rates\n",
    "    * Change from small to large: hopefully in 'good' region long enough before diverging\n",
    "    * Warm restarts: aggressive decay + reset to initial learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def compare_optimizers(iterations=(1,100,1), optimizer1=opt_names, optimizer2=opt_names):\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    plot_optimizers(ax,iterations,[optimizer1,optimizer2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    fig, axes = plt.subplots(1,2, figsize=(10,3))\n",
    "    optimizers = ['sgd_decay', 'sgd_cyclic']\n",
    "    for function, ax in zip(optimizers,axes):\n",
    "        plot_optimizers(ax,100,function)\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Momentum\n",
    "* Imagine a ball rolling downhill: accumulates momentum, doesn't exactly follow steepest descent\n",
    "    * Reduces oscillation, follows larger (consistent) gradient of the loss surface\n",
    "* Adds a velocity vector $\\mathbf{v}$ with momentum $\\gamma$ (e.g. 0.9, or increase from $\\gamma=0.5$ to $\\gamma=0.99$)\n",
    "$$\\mathbf{w}_{(s+1)} = \\mathbf{w}_{(s)} + \\mathbf{v}_{(s)} \\qquad \\text{with} \\qquad\n",
    "\\color{blue}{\\mathbf{v}_{(s)}} = \\color{green}{\\gamma \\mathbf{v}_{(s-1)}} - \\color{red}{\\eta \\nabla \\mathcal{L}(\\mathbf{w}_{(s)})}$$\n",
    "* Nesterov momentum: Look where momentum step would bring you, compute gradient there \n",
    "    * Responds faster (and reduces momentum) when the gradient changes\n",
    "$$\\color{blue}{\\mathbf{v}_{(s)}} = \\color{green}{\\gamma \\mathbf{v}_{(s-1)}} - \\color{red}{\\eta \\nabla \\mathcal{L}(\\mathbf{w}_{(s)} + \\gamma \\mathbf{v}_{(s-1)})}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(10,2.6))\n",
    "plot_nesterov(axes[0],method=\"Momentum\")\n",
    "plot_nesterov(axes[1],method=\"Nesterov momentum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Momentum in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def compare_optimizers(iterations=(1,100,1), optimizer1=opt_names, optimizer2=opt_names):\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    plot_optimizers(ax,iterations,[optimizer1,optimizer2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    fig, axes = plt.subplots(1,2, figsize=(10,3.5))\n",
    "    optimizers = [['sgd','momentum'], ['momentum','nesterov']]\n",
    "    for function, ax in zip(optimizers,axes):\n",
    "        plot_optimizers(ax,100,function)\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Adaptive gradients\n",
    "* 'Correct' the learning rate for each $w_i$ based on specific local conditions (layer depth, fan-in,...)\n",
    "* Adagrad: scale $\\eta$ according to squared sum of previous gradients $G_{i,(s)} = \\sum_{t=1}^s \\mathcal{L}(w_{i,(t)})^2$\n",
    "    * Update rule for $w_i$. Usually $\\epsilon=10^{-7}$ (avoids division by 0), $\\eta=0.001$.\n",
    "$$w_{i,(s+1)} = w_{i,(s)} - \\frac{\\eta}{\\sqrt{G_{i,(s)}+\\epsilon}} \\nabla \\mathcal{L}(w_{i,(s)})$$\n",
    "* RMSProp: use _moving average_ of squared gradients $m_{i,(s)} = \\gamma m_{i,(s-1)} + (1-\\gamma) \\nabla \\mathcal{L}(w_{i,(s)})^2$\n",
    "    * Avoids that gradients dwindle to 0 as $G_{i,(s)}$ grows. Usually $\\gamma=0.9, \\eta=0.001$\n",
    "$$w_{i,(s+1)} = w_{i,(s)}- \\frac{\\eta}{\\sqrt{m_{i,(s)}+\\epsilon}} \\nabla \\mathcal{L}(w_{i,(s)})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    fig, axes = plt.subplots(1,2, figsize=(10,2.6))\n",
    "    optimizers = [['sgd','adagrad', 'rmsprop'], ['rmsprop','rmsprop_mom']]\n",
    "    for function, ax in zip(optimizers,axes):\n",
    "        plot_optimizers(ax,100,function)\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def compare_optimizers(iterations=(1,100,1), optimizer1=opt_names, optimizer2=opt_names):\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    plot_optimizers(ax,iterations,[optimizer1,optimizer2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Adam (Adaptive moment estimation)\n",
    "* Adam: RMSProp + momentum. Adds moving average for gradients as well ($\\gamma_2$ = momentum): \n",
    "    * Adds a bias correction to avoid small initial gradients: $\\hat{m}_{i,(s)} = \\frac{m_{i,(s)}}{1-\\gamma}$ and $\\hat{g}_{i,(s)} = \\frac{g_{i,(s)}}{1-\\gamma_2}$\n",
    "    $$g_{i,(s)} = \\gamma_2 g_{i,(s-1)} + (1-\\gamma_2) \\nabla \\mathcal{L}(w_{i,(s)})$$\n",
    "    $$w_{i,(s+1)} = w_{i,(s)}- \\frac{\\eta}{\\sqrt{\\hat{m}_{i,(s)}+\\epsilon}} \\hat{g}_{i,(s)}$$\n",
    "\n",
    "* Adamax: Idem, but use max() instead of moving average: $u_{i,(s)} = max(\\gamma u_{i,(s-1)}, |\\mathcal{L}(w_{i,(s)})|)$\n",
    "$$w_{i,(s+1)} = w_{i,(s)}- \\frac{\\eta}{u_{i,(s)}} \\hat{g}_{i,(s)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    fig, axes = plt.subplots(1,2, figsize=(10,2.6))\n",
    "    optimizers = [['sgd','adam'], ['adam','adamax']]\n",
    "    for function, ax in zip(optimizers,axes):\n",
    "        plot_optimizers(ax,100,function)\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def compare_optimizers(iterations=(1,100,1), optimizer1=opt_names, optimizer2=opt_names):\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    plot_optimizers(ax,iterations,[optimizer1,optimizer2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SGD Optimizer Zoo\n",
    "* RMSProp often works well, but do try alternatives. For even more optimizers, [see here](https://ruder.io/optimizing-gradient-descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    fig, ax = plt.subplots(1,1, figsize=(10,5.5))\n",
    "    plot_optimizers(ax,100,opt_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def compare_optimizers(iterations=(1,100,1)):\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    plot_optimizers(ax,iterations,opt_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from numpy.random import seed\n",
    "from tensorflow.random import set_seed\n",
    "import random\n",
    "import os\n",
    "\n",
    "#Trying to set all seeds\n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "random.seed(0)\n",
    "seed(0)\n",
    "set_seed(0)\n",
    "seed_value= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural networks in practice\n",
    "* There are many practical courses on training neural nets. E.g.:\n",
    "    * With TensorFlow: https://www.tensorflow.org/resources/learn-ml\n",
    "    * With PyTorch: [fast.ai course](https://course.fast.ai/), https://pytorch.org/tutorials/\n",
    "* Here, we'll use Keras, a general API for building neural networks\n",
    "    * Default API for TensorFlow, also has backends for CNTK, Theano\n",
    "* Focus on key design decisions, evaluation, and regularization\n",
    "* Running example: Fashion-MNIST\n",
    "    * 28x28 pixel images of 10 classes of fashion items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# Download FMINST data. Takes a while the first time.\n",
    "mnist = oml.datasets.get_dataset(40996)\n",
    "X, y, _, _ = mnist.get_data(target=mnist.default_target_attribute, dataset_format='array');\n",
    "X = X.reshape(70000, 28, 28)\n",
    "fmnist_classes = {0:\"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\", 5: \"Sandal\", \n",
    "                  6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle boot\"}\n",
    "\n",
    "# Take some random examples\n",
    "from random import randint\n",
    "fig, axes = plt.subplots(1, 5,  figsize=(10, 5))\n",
    "for i in range(5):\n",
    "    n = randint(0,70000)\n",
    "    axes[i].imshow(X[n], cmap=plt.cm.gray_r)\n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "    axes[i].set_xlabel(\"{}\".format(fmnist_classes[y[n]]))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building the network\n",
    "* We first build a simple sequential model (no branches)\n",
    "* Input layer ('input_shape'): a flat vector of 28*28=784 nodes\n",
    "    * We'll see how to properly deal with images later\n",
    "* Two dense hidden layers: 512 nodes each, ReLU activation\n",
    "    * Glorot weight initialization is applied by default\n",
    "* Output layer: 10 nodes (for 10 classes) and softmax activation\n",
    "\n",
    "``` python\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', kernel_initializer='he_normal', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(512, activation='relu', kernel_initializer='he_normal'))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import initializers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', kernel_initializer='he_normal', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(512, activation='relu', kernel_initializer='he_normal'))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Model summary\n",
    "- Lots of parameters (weights and biases) to learn!\n",
    "    - hidden layer 1 : (28 * 28 + 1) * 512 = 401920\n",
    "    - hidden layer 2 : (512 + 1) * 512 = 262656\n",
    "    - output layer: (512 + 1) * 10 = 5130\n",
    "    \n",
    "``` python\n",
    "network.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choosing loss, optimizer, metrics\n",
    "* __Loss function__\n",
    "    - Cross-entropy (log loss) for multi-class classification ($y_{true}$ is one-hot encoded)\n",
    "    - Use binary crossentropy for binary problems (single output node) \n",
    "    - Use sparse categorical crossentropy if $y_{true}$ is label-encoded (1,2,3,...)\n",
    "* __Optimizer__\n",
    "    - Any of the optimizers we discussed before. RMSprop usually works well.\n",
    "* __Metrics__ \n",
    "    - To monitor performance during training and testing, e.g. accuracy\n",
    "    \n",
    "``` python\n",
    "# Shorthand\n",
    "network.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "# Detailed\n",
    "network.compile(loss=CategoricalCrossentropy(label_smoothing=0.01),\n",
    "                optimizer=RMSprop(learning_rate=0.001, momentum=0.0)\n",
    "                metrics=[Accuracy()])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "\n",
    "network.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preprocessing: Normalization, Reshaping, Encoding \n",
    "* Always normalize (standardize or min-max) the inputs. Mean should be close to 0.\n",
    "    - Avoid that some inputs overpower others\n",
    "    - Speed up convergence \n",
    "        - Gradients of activation functions $\\frac{\\partial a_{h}}{\\partial z_{h}}$ are (near) 0 for large inputs\n",
    "        - If some gradients become much larger than others, SGD will start zig-zagging\n",
    "* Reshape the data to fit the shape of the input layer, e.g. (n, 28*28) or (n, 28,28)\n",
    "    - Tensor with instances in first dimension, rest must match the input layer \n",
    "* In multi-class classification, every class is an output node, so one-hot-encode the labels\n",
    "    - e.g. class '4' becomes [0,0,0,0,1,0,0,0,0,0]\n",
    "    \n",
    "```python\n",
    "X = X.astype('float32') / 255\n",
    "X = X.reshape((60000, 28 * 28))\n",
    "y = to_categorical(y)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xf_train, Xf_test, yf_train, yf_test = train_test_split(X, y, train_size=60000, shuffle=True, random_state=0)\n",
    "\n",
    "Xf_train = Xf_train.reshape((60000, 28 * 28))\n",
    "Xf_test = Xf_test.reshape((10000, 28 * 28))\n",
    "\n",
    "# TODO: check if standardization works better\n",
    "Xf_train = Xf_train.astype('float32') / 255\n",
    "Xf_test = Xf_test.astype('float32') / 255\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "yf_train = to_categorical(yf_train)\n",
    "yf_test = to_categorical(yf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choosing training hyperparameters\n",
    "* Number of epochs: enough to allow convergence\n",
    "    * Too much: model starts overfitting (or just wastes time)\n",
    "* Batch size: small batches (e.g. 32, 64,... samples) often preferred\n",
    "    * 'Noisy' training data makes overfitting less likely\n",
    "        * Larger batches generalize less well ('generalization gap')\n",
    "    * Requires less memory (especially in GPUs)\n",
    "    * Large batches do speed up training, may converge in fewer epochs\n",
    "* [Batch size interacts with learning rate](https://openreview.net/pdf?id=B1Yy1BxCZ)\n",
    "    * Instead of shrinking the learning rate you can increase batch size\n",
    "    \n",
    "``` python\n",
    "history = network.fit(X_train, y_train, epochs=3, batch_size=32);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "history = network.fit(Xf_train, yf_train, epochs=3, batch_size=32);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Predictions and evaluations\n",
    "We can now call `predict` to generate predictions, and evaluate the trained model on the entire test set\n",
    "\n",
    "``` python\n",
    "network.predict(X_test)\n",
    "test_loss, test_acc = network.evaluate(X_test, y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=7)\n",
    "fig, axes = plt.subplots(1, 1, figsize=(2, 2))\n",
    "sample_id = 4\n",
    "axes.imshow(Xf_test[sample_id].reshape(28, 28), cmap=plt.cm.gray_r)\n",
    "axes.set_xlabel(\"True label: {}\".format(yf_test[sample_id]))\n",
    "axes.set_xticks([])\n",
    "axes.set_yticks([])\n",
    "print(network.predict(Xf_test)[sample_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = network.evaluate(Xf_test, yf_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model selection\n",
    "* How many epochs do we need for training?\n",
    "* Train the neural net and track the loss after every iteration on a validation set\n",
    "    * You can add a callback to the fit version to get info on every epoch\n",
    "* Best model after a few epochs, then starts overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# For plotting the learning curve in real time\n",
    "class TrainingPlot(Callback):\n",
    "    \n",
    "    # This function is called when the training begins\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # Initialize the lists for holding the logs, losses and accuracies\n",
    "        self.losses = []\n",
    "        self.acc = []\n",
    "        self.val_losses = []\n",
    "        self.val_acc = []\n",
    "        self.logs = []\n",
    "        self.max_acc = 0\n",
    "    \n",
    "    # This function is called at the end of each epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        # Append the logs, losses and accuracies to the lists\n",
    "        self.logs.append(logs)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.acc.append(logs.get('accuracy'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.val_acc.append(logs.get('val_accuracy'))\n",
    "        self.max_acc = max(self.max_acc, logs.get('val_accuracy'))\n",
    "        \n",
    "        # Before plotting ensure at least 2 epochs have passed\n",
    "        if len(self.losses) > 1:\n",
    "            \n",
    "            # Clear the previous plot\n",
    "            clear_output(wait=True)\n",
    "            N = np.arange(0, len(self.losses))\n",
    "            \n",
    "            # Plot train loss, train acc, val loss and val acc against epochs passed\n",
    "            plt.figure(figsize=(8,3))\n",
    "            plt.plot(N, self.losses, lw=2, c=\"b\", linestyle=\"-\", label = \"train_loss\")\n",
    "            plt.plot(N, self.acc, lw=2, c=\"r\", linestyle=\"-\", label = \"train_acc\")\n",
    "            plt.plot(N, self.val_losses, lw=2, c=\"b\", linestyle=\":\", label = \"val_loss\")\n",
    "            plt.plot(N, self.val_acc, lw=2, c=\"r\", linestyle=\":\", label = \"val_acc\")\n",
    "            plt.title(\"Training Loss and Accuracy [Epoch {}, Max Acc {:.4f}]\".format(epoch, self.max_acc))\n",
    "            plt.xlabel(\"Epoch #\")\n",
    "            plt.ylabel(\"Loss/Accuracy\")\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_val, partial_x_train = Xf_train[:10000], Xf_train[10000:]\n",
    "y_val, partial_y_train = yf_train[:10000], yf_train[10000:] \n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', kernel_initializer='he_normal', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(512, activation='relu', kernel_initializer='he_normal'))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "network.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "plot_losses = TrainingPlot()\n",
    "history = network.fit(partial_x_train, partial_y_train, epochs=25, batch_size=512, verbose=0,\n",
    "                      validation_data=(x_val, y_val), callbacks=[plot_losses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Early stopping\n",
    "* Stop training when the validation loss (or validation accuracy) no longer improves\n",
    "* Loss can be bumpy: use a moving average or wait for $k$ steps without improvement\n",
    "\n",
    "``` python\n",
    "earlystop = callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "model.fit(x_train, y_train, epochs=25, batch_size=512, callbacks=[earlystop])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "\n",
    "earlystop = callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', kernel_initializer='he_normal', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(512, activation='relu', kernel_initializer='he_normal'))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "network.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "plot_losses = TrainingPlot()\n",
    "history = network.fit(partial_x_train, partial_y_train, epochs=25, batch_size=512, verbose=0,\n",
    "                      validation_data=(x_val, y_val), callbacks=[plot_losses, earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularization and memorization capacity\n",
    "* The number of learnable parameters is called the model _capacity_\n",
    "* A model with more parameters has a higher _memorization capacity_\n",
    "    - Too high capacity causes overfitting, too low causes underfitting\n",
    "    - In the extreme, the training set can be 'memorized' in the weights\n",
    "* Smaller models are forced it to learn a compressed representation that generalizes better\n",
    "    - Find the sweet spot: e.g. start with few parameters, increase until overfitting stars.\n",
    "* Example: 256 nodes in first layer, 32 nodes in second layer, similar performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(256, activation='relu', kernel_initializer='he_normal', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(32, activation='relu', kernel_initializer='he_normal'))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "network.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "earlystop5 = callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "plot_losses = TrainingPlot()\n",
    "history = network.fit(partial_x_train, partial_y_train, epochs=30, batch_size=512, verbose=0,\n",
    "                      validation_data=(x_val, y_val), callbacks=[plot_losses, earlystop5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Information bottleneck\n",
    "* If a layer is too narrow, it will lose information that can never be recovered by subsequent layers\n",
    "* _Information bottleneck_ theory defines a bound on the capacity of the network\n",
    "* Imagine that you need to learn 10 outputs (e.g. classes) and your hidden layer has 2 nodes\n",
    "    * This is like trying to learn 10 hyperplanes from a 2-dimensional representation\n",
    "* Example: bottleneck of 2 nodes, no overfitting, much higher training loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(256, activation='relu', kernel_initializer='he_normal', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(2, activation='relu', kernel_initializer='he_normal'))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "network.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "earlystop5 = callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "plot_losses = TrainingPlot()\n",
    "history = network.fit(partial_x_train, partial_y_train, epochs=30, batch_size=512, verbose=0,\n",
    "                      validation_data=(x_val, y_val), callbacks=[plot_losses, earlystop5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Weight regularization (weight decay)\n",
    "* As we did many times before, we can also add weight regularization to our loss function\n",
    "- L1 regularization: leads to _sparse networks_ with many weights that are 0\n",
    "- L2 regularization: leads to many very small weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "network.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "earlystop5 = callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "plot_losses = TrainingPlot()\n",
    "history = network.fit(partial_x_train, partial_y_train, epochs=50, batch_size=512, verbose=0,\n",
    "                      validation_data=(x_val, y_val), callbacks=[plot_losses, earlystop5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dropout\n",
    "* Every iteration, randomly set a number of activations $a_i$ to 0\n",
    "* _Dropout rate_ : fraction of the outputs that are zeroed-out (e.g. 0.1 - 0.5)\n",
    "* Idea: break up accidental non-significant learned patterns \n",
    "* At test time, nothing is dropped out, but the output values are scaled down by the dropout rate\n",
    "    - Balances out that more units are active than during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.gca()\n",
    "draw_neural_net(ax, [2, 3, 1], draw_bias=True, labels=True, \n",
    "                show_activations=True, activation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Dropout layers\n",
    "* Dropout is usually implemented as a special layer\n",
    "\n",
    "``` python\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(256, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(32, activation='relu'))\n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(256, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dropout(0.3))\n",
    "network.add(layers.Dense(32, activation='relu'))\n",
    "network.add(layers.Dropout(0.3))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "network.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "plot_losses = TrainingPlot()\n",
    "history = network.fit(partial_x_train, partial_y_train, epochs=50, batch_size=512, verbose=0,\n",
    "                      validation_data=(x_val, y_val), callbacks=[plot_losses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Batch Normalization\n",
    "* We've seen that scaling the input is important, but what if layer activations become very large? \n",
    "    * Same problems, starting deeper in the network\n",
    "* Batch normalization: normalize the activations of the previous layer within each batch\n",
    "    * Within a batch, set the mean activation close to 0 and the standard deviation close to 1\n",
    "        * Across badges, use exponential moving average of batch-wise mean and variance\n",
    "    * Allows deeper networks less prone to vanishing or exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.BatchNormalization())\n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(256, activation='relu'))\n",
    "network.add(layers.BatchNormalization())\n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(64, activation='relu'))\n",
    "network.add(layers.BatchNormalization())\n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(32, activation='relu'))\n",
    "network.add(layers.BatchNormalization())\n",
    "network.add(layers.Dropout(0.5))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(265, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.BatchNormalization())\n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(64, activation='relu'))\n",
    "network.add(layers.BatchNormalization())\n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(32, activation='relu'))\n",
    "network.add(layers.BatchNormalization())\n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "network.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "plot_losses = TrainingPlot()\n",
    "history = network.fit(partial_x_train, partial_y_train, epochs=50, batch_size=512, verbose=0,\n",
    "                      validation_data=(x_val, y_val), callbacks=[plot_losses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tuning multiple hyperparameters\n",
    "* You can wrap Keras models as scikit-learn models and use any tuning technique\n",
    "* Keras also has built-in RandomSearch (and HyperBand and BayesianOptimization - see later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def make_model(hp):\n",
    "    m.add(Dense(units=hp.Int('units', min_value=32, max_value=512, step=32)))\n",
    "    m.compile(optimizer=Adam(hp.Choice('learning rate', [1e-2, 1e-3, 1e-4])))\n",
    "    return model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "clf = KerasClassifier(make_model)\n",
    "grid = GridSearchCV(clf, param_grid=param_grid, cv=3)\n",
    "\n",
    "from kerastuner.tuners import RandomSearch\n",
    "tuner = keras.RandomSearch(build_model, max_trials=5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "* Neural architectures\n",
    "* Training neural nets\n",
    "    * Forward pass: Tensor operations\n",
    "    * Backward pass: Backpropagation\n",
    "* Neural network design:\n",
    "    * Activation functions\n",
    "    * Weight initialization\n",
    "    * Optimizers\n",
    "* Neural networks in practice\n",
    "* Model selection\n",
    "    * Early stopping\n",
    "    * Memorization capacity and information bottleneck\n",
    "    * L1/L2 regularization\n",
    "    * Dropout\n",
    "    * Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
