{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VU3bkm1YYidb"
   },
   "source": [
    "# Lecture 9: Convolutional Neural Networks\n",
    "\n",
    "**Handling image data**\n",
    "\n",
    "Joaquin Vanschoren, Eindhoven University of Technology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyAIooipYidn"
   },
   "source": [
    "## Overview\n",
    "\n",
    "* Image convolution\n",
    "* Convolutional neural networks\n",
    "* Data augmentation\n",
    "* Model interpretation\n",
    "* Using pre-trained networks (transfer learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "czkUG-r4Yidn"
   },
   "outputs": [],
   "source": [
    "# Global imports and settings\n",
    "from preamble import *\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel(3)\n",
    "print(\"Using Keras\",tf.keras.__version__)\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 300 # Use 300 for PDF, 100 for slides\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "HTML('''<style>html, body{overflow-y: visible !important} .CodeMirror{min-width:100% !important;} .rise-enabled .CodeMirror, .rise-enabled .output_subarea{font-size:140%; line-height:1.2; overflow: visible;} .output_subarea pre{width:110%}</style>''') # For slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvofryp9Yido"
   },
   "outputs": [],
   "source": [
    "base_dir = '../data/cats-vs-dogs_small'\n",
    "model_dir = '../data/models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V645afu_Yido"
   },
   "source": [
    "**Note: if you want to run this notebook in Google Colab on a GPU:**\n",
    "- upload the `data` folder to your Google Drive\n",
    "- In Colab, left menu > Files, mount your Drive\n",
    "- Change the above code to\n",
    "```python\n",
    "base_dir = '/content/drive/My Drive/data/cats-vs-dogs_small'\n",
    "model_dir = '/content/drive/My Drive/data/models'\n",
    "```\n",
    "- Under Edit > Notebook Settings, choose Hardware Accelerator: GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5gSmtvqYidp"
   },
   "source": [
    "### Convolution\n",
    "* Operation that transforms an image by sliding a smaller image (called a _filter_ or _kernel_ ) over the image and multiplying the pixel values\n",
    "    * Slide an $n$ x $n$ filter over $n$ x $n$ _patches_ of the original image\n",
    "    * Every pixel is replaced by the _sum_ of the _element-wise products_ of the values of the image patch around that pixel and the kernel \n",
    "\n",
    "``` python\n",
    "# kernel and image_patch are n x n matrices\n",
    "pixel_out = np.sum(kernel * image_patch)\n",
    "```\n",
    "\n",
    "<img src=\"https://github.com/ML-course/master/blob/master/images/10_conv_filtering.png?raw=1\" alt=\"ml\" style=\"width: 500px; margin-left: auto; margin-right: auto;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNw-a9gjYidp"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from skimage import color\n",
    "\n",
    "\n",
    "# Visualize convolution. See https://tonysyu.github.io/\n",
    "def iter_pixels(image):\n",
    "    \"\"\" Yield pixel position (row, column) and pixel intensity. \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            yield (i, j), image[i, j]\n",
    "            \n",
    "# Visualize result\n",
    "def imshow_pair(image_pair, titles=('', ''), figsize=(10, 5), **kwargs):\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=figsize)\n",
    "    for ax, img, label in zip(axes.ravel(), image_pair, titles):\n",
    "        ax.imshow(img, **kwargs)\n",
    "        ax.set_title(label)\n",
    "        \n",
    "# Visualize result\n",
    "def imshow_triple(axes, image_pair, titles=('', '', ''), figsize=(10, 5), **kwargs):\n",
    "    for ax, img, label in zip(axes, image_pair, titles):\n",
    "        ax.imshow(img, **kwargs)\n",
    "        ax.set_title(label, fontdict={'fontsize':8})\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "# Zero-padding\n",
    "def padding_for_kernel(kernel):\n",
    "    \"\"\" Return the amount of padding needed for each side of an image.\n",
    "\n",
    "    For example, if the returned result is [1, 2], then this means an\n",
    "    image should be padded with 1 extra row on top and bottom, and 2\n",
    "    extra columns on the left and right.\n",
    "    \"\"\"\n",
    "    # Slice to ignore RGB channels if they exist.\n",
    "    image_shape = kernel.shape[:2]\n",
    "    # We only handle kernels with odd dimensions so make sure that's true.\n",
    "    # (The \"center\" pixel of an even number of pixels is arbitrary.)\n",
    "    assert all((size % 2) == 1 for size in image_shape)\n",
    "    return [(size - 1) // 2 for size in image_shape]\n",
    "def add_padding(image, kernel):\n",
    "    h_pad, w_pad = padding_for_kernel(kernel)\n",
    "    return np.pad(image, ((h_pad, h_pad), (w_pad, w_pad)),\n",
    "                  mode='constant', constant_values=0)\n",
    "def remove_padding(image, kernel):\n",
    "    inner_region = []  # A 2D slice for grabbing the inner image region\n",
    "    for pad in padding_for_kernel(kernel):\n",
    "        slice_i = slice(None) if pad == 0 else slice(pad, -pad)\n",
    "        inner_region.append(slice_i)\n",
    "    return image[inner_region]\n",
    "\n",
    "# Slice windows\n",
    "def window_slice(center, kernel):\n",
    "    r, c = center\n",
    "    r_pad, c_pad = padding_for_kernel(kernel)\n",
    "    # Slicing is (inclusive, exclusive) so add 1 to the stop value\n",
    "    return [slice(r-r_pad, r+r_pad+1), slice(c-c_pad, c+c_pad+1)]\n",
    "\n",
    "# Apply convolution kernel to image patch\n",
    "def apply_kernel(center, kernel, original_image):\n",
    "    image_patch = original_image[window_slice(center, kernel)]\n",
    "    # An element-wise multiplication followed by the sum\n",
    "    return np.sum(kernel * image_patch)\n",
    "\n",
    "# Move kernel over the image\n",
    "def iter_kernel_labels(image, kernel):\n",
    "    original_image = image\n",
    "    image = add_padding(original_image, kernel)\n",
    "    i_pad, j_pad = padding_for_kernel(kernel)\n",
    "\n",
    "    for (i, j), pixel in iter_pixels(original_image):\n",
    "        # Shift the center of the kernel to ignore padded border.\n",
    "        i += i_pad\n",
    "        j += j_pad\n",
    "        mask = np.zeros(image.shape, dtype=int)  # Background = 0\n",
    "        mask[window_slice((i, j), kernel)] = kernel   # Kernel = 1\n",
    "        #mask[i, j] = 2                           # Kernel-center = 2\n",
    "        yield (i, j), mask\n",
    "\n",
    "# Visualize kernel as it moves over the image\n",
    "def visualize_kernel(kernel_labels, image):\n",
    "    return kernel_labels + image #color.label2rgb(kernel_labels, image, bg_label=0)\n",
    "\n",
    "# Do a single step\n",
    "def convolution_demo(image, kernel, **kwargs):\n",
    "    # Initialize generator since we're only ever going to iterate over\n",
    "    # a pixel once. The cached result is used, if we step back.\n",
    "    gen_kernel_labels = iter_kernel_labels(image, kernel)\n",
    "\n",
    "    image_cache = []\n",
    "    image_padded = add_padding(image, kernel)\n",
    "    # Plot original image and kernel-overlay next to filtered image.\n",
    "    @interact(i_step=(0, image.size-1,1))\n",
    "    def convolution_step(i_step=0):\n",
    "        # Create all images up to the current step, unless they're already\n",
    "        # cached:\n",
    "        while i_step >= len(image_cache):\n",
    "\n",
    "            # For the first step (`i_step == 0`), the original image is the\n",
    "            # filtered image; after that we look in the cache, which stores\n",
    "            # (`kernel_overlay`, `filtered`).\n",
    "            filtered_prev = image_padded if i_step == 0 else image_cache[-1][1]\n",
    "            # We don't want to overwrite the previously filtered image:\n",
    "            filtered = filtered_prev.copy()\n",
    "\n",
    "            # Get the labels used to visualize the kernel\n",
    "            center, kernel_labels = next(gen_kernel_labels)\n",
    "            # Modify the pixel value at the kernel center\n",
    "            filtered[center] = apply_kernel(center, kernel, image_padded)\n",
    "            # Take the original image and overlay our kernel visualization\n",
    "            kernel_overlay = visualize_kernel(kernel_labels, image_padded)\n",
    "            # Save images for reuse.\n",
    "            image_cache.append((kernel_overlay, filtered))\n",
    "\n",
    "        # Remove padding we added to deal with boundary conditions\n",
    "        # (Loop since each step has 2 images)\n",
    "        image_pair = [remove_padding(each, kernel)\n",
    "                      for each in image_cache[i_step]]\n",
    "        imshow_pair(image_pair, **kwargs)\n",
    "        plt.show()\n",
    "\n",
    "    return convolution_step\n",
    "\n",
    "# Full process\n",
    "def convolution_full(ax, image, kernel, **kwargs):\n",
    "    # Initialize generator since we're only ever going to iterate over\n",
    "    # a pixel once. The cached result is used, if we step back.\n",
    "    gen_kernel_labels = iter_kernel_labels(image, kernel)\n",
    "\n",
    "    image_cache = []\n",
    "    image_padded = add_padding(image, kernel)\n",
    "    # Plot original image and kernel-overlay next to filtered image.\n",
    "\n",
    "    for i_step in range(image.size-1):\n",
    "\n",
    "        # For the first step (`i_step == 0`), the original image is the\n",
    "        # filtered image; after that we look in the cache, which stores\n",
    "        # (`kernel_overlay`, `filtered`).\n",
    "        filtered_prev = image_padded if i_step == 0 else image_cache[-1][1]\n",
    "        # We don't want to overwrite the previously filtered image:\n",
    "        filtered = filtered_prev.copy()\n",
    "\n",
    "        # Get the labels used to visualize the kernel\n",
    "        center, kernel_labels = next(gen_kernel_labels)\n",
    "        # Modify the pixel value at the kernel center\n",
    "        filtered[center] = apply_kernel(center, kernel, image_padded)\n",
    "        # Take the original image and overlay our kernel visualization\n",
    "        kernel_overlay = visualize_kernel(kernel_labels, image_padded)\n",
    "        # Save images for reuse.\n",
    "        image_cache.append((kernel_overlay, filtered))\n",
    "\n",
    "    # Remove padding we added to deal with boundary conditions\n",
    "    # (Loop since each step has 2 images)\n",
    "    image_triple = [remove_padding(each, kernel)\n",
    "                  for each in image_cache[i_step]]\n",
    "    image_triple.insert(1,kernel)\n",
    "    imshow_triple(ax, image_triple, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFN4VgubYids"
   },
   "source": [
    "* Different kernels can detect different types of patterns in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "id": "UVcQRmuXYids"
   },
   "outputs": [],
   "source": [
    "horizontal_edge_kernel = np.array([[ 1,  2,  1],\n",
    "                                   [ 0,  0,  0],\n",
    "                                   [-1, -2, -1]])\n",
    "diagonal_edge_kernel = np.array([[1, 0, 0],\n",
    "                                 [0, 1, 0],\n",
    "                                 [0, 0, 1]])\n",
    "edge_detect_kernel = np.array([[-1, -1, -1],\n",
    "                               [-1,  8, -1],\n",
    "                               [-1, -1, -1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaDdnJjPYidt"
   },
   "source": [
    "Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25ElHFEQYidt"
   },
   "outputs": [],
   "source": [
    "mnist_data = oml.datasets.get_dataset(554) # Download MNIST data\n",
    "# Get the predictors X and the labels y\n",
    "X_mnist, y_mnist, c, a = mnist_data.get_data(dataset_format='array', target=mnist_data.default_target_attribute); \n",
    "image = X_mnist[1].reshape((28, 28))\n",
    "image = (image - np.min(image))/np.ptp(image) # Normalize\n",
    "\n",
    "titles = ('Image and kernel', 'Filtered image')\n",
    "convolution_demo(image, horizontal_edge_kernel, vmin=-4, vmax=4, titles=titles, cmap='gray_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Kp1dGzJYidt"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 150\n",
    "fig, axs = plt.subplots(3, 3)\n",
    "titles = ('Image and kernel', 'Hor. edge filter', 'Filtered image')\n",
    "convolution_full(axs[0,:], image, horizontal_edge_kernel, vmin=-4, vmax=4, titles=titles, cmap='gray_r')\n",
    "titles = ('Image and kernel', 'Edge detect filter', 'Filtered image')\n",
    "convolution_full(axs[1,:], image, edge_detect_kernel, vmin=-4, vmax=4, titles=titles, cmap='gray_r')\n",
    "titles = ('Image and kernel', 'Diag. edge filter', 'Filtered image')\n",
    "convolution_full(axs[2,:], image, diagonal_edge_kernel, vmin=-4, vmax=4, titles=titles, cmap='gray_r')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "greXxJk-Yidt"
   },
   "source": [
    "### Demonstration on Google streetview data\n",
    "House numbers photographed from Google streetview imagery, cropped and centered around digits, but with neighboring numbers or other edge artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_O2UWkyYidt"
   },
   "outputs": [],
   "source": [
    "SVHN = oml.datasets.get_dataset(41081)\n",
    "X, y, cats, attrs = SVHN.get_data(dataset_format='array',\n",
    "    target=SVHN.default_target_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LCiUiY2Yidu"
   },
   "outputs": [],
   "source": [
    "def plot_images(X, y, grayscale=False):\n",
    "    fig, axes = plt.subplots(1, len(X),  figsize=(10, 5))\n",
    "    for n in range(len(X)):\n",
    "        if grayscale:\n",
    "            axes[n].imshow(X[n].reshape(32, 32)/255, cmap='gray')\n",
    "        else:\n",
    "            axes[n].imshow(X[n].reshape(32, 32, 3)/255)\n",
    "        axes[n].set_xlabel((y[n]+1)) # Label is index+1\n",
    "        axes[n].set_xticks(()), axes[n].set_yticks(())\n",
    "    plt.show();\n",
    "\n",
    "images = range(5)\n",
    "X_sub_color = [X[i] for i in images]\n",
    "y_sub = [y[i] for i in images]\n",
    "plt.rcParams['figure.dpi'] = 60\n",
    "plot_images(X_sub_color, y_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eAK8ERGYidu"
   },
   "source": [
    "For recognizing digits, color is not important, so we grayscale the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_vwFrO7Yidu"
   },
   "outputs": [],
   "source": [
    "def rgb2gray(X, dim=32):\n",
    "    return np.expand_dims(np.dot(X.reshape(len(X), dim*dim, 3), [0.2990, 0.5870, 0.1140]), axis=2)\n",
    "Xsm = rgb2gray(X[:100])\n",
    "\n",
    "X_sub = [Xsm[i] for i in images]\n",
    "plot_images(X_sub, y_sub, grayscale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwPhhtmYYidu"
   },
   "source": [
    "Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "id": "2mx8X_PtYidu"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 100\n",
    "def normalize_image(X):\n",
    "    image = X.reshape((32, 32))\n",
    "    return (image - np.min(image))/np.ptp(image) # Normalize\n",
    "\n",
    "image = normalize_image(X_sub[3])\n",
    "demo2 = convolution_demo(image, horizontal_edge_kernel,\n",
    "                 vmin=-4, vmax=4, cmap='gray_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hV7eNvGYidu"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 150\n",
    "fig, axs = plt.subplots(3, 3)\n",
    "titles = ('Image and kernel', 'Hor. edge filter', 'Filtered image')\n",
    "convolution_full(axs[0,:], image, horizontal_edge_kernel, vmin=-4, vmax=4, titles=titles, cmap='gray_r')\n",
    "titles = ('Image and kernel', 'Diag. edge filter', 'Filtered image')\n",
    "convolution_full(axs[1,:], image, diagonal_edge_kernel, vmin=-4, vmax=4, titles=titles, cmap='gray_r')\n",
    "titles = ('Image and kernel', 'Edge detect filter', 'Filtered image')\n",
    "convolution_full(axs[2,:], image, edge_detect_kernel, vmin=-4, vmax=4, titles=titles, cmap='gray_r')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYOcaGH5Yidv"
   },
   "source": [
    "### Image convolution in practice\n",
    "* How do we know which filters are best for a given image?\n",
    "* _Families_ of kernels (or _filter banks_ ) can be run on every image\n",
    "    * Gabor, Sobel, Haar Wavelets,...\n",
    "* Gabor filters: Wave patterns generated by changing:\n",
    "    - Frequency: narrow or wide ondulations\n",
    "    - Theta: angle (direction) of the wave\n",
    "    - Sigma: resolution (size of the filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZczPiBggYidv"
   },
   "source": [
    "Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "ohEo8-B9Yidv"
   },
   "outputs": [],
   "source": [
    "from scipy import ndimage as ndi\n",
    "from skimage import data\n",
    "from skimage.util import img_as_float\n",
    "from skimage.filters import gabor_kernel\n",
    "\n",
    "# Gabor Filters.\n",
    "@interact\n",
    "def demoGabor(frequency=(0.01,1,0.05), theta=(0,3.14,0.1), sigma=(0,5,0.1)):\n",
    "    plt.gray()\n",
    "    plt.imshow(np.real(gabor_kernel(frequency=frequency, theta=theta, sigma_x=sigma, sigma_y=sigma)), interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLRU2LidYidv"
   },
   "outputs": [],
   "source": [
    "plt.subplot(1, 3, 1)\n",
    "demoGabor(frequency=0.16, theta=1.2, sigma=4.0)\n",
    "plt.subplot(1, 3, 2)\n",
    "demoGabor(frequency=0.31, theta=0, sigma=3.6)\n",
    "plt.subplot(1, 3, 3)\n",
    "demoGabor(frequency=0.36, theta=1.6, sigma=1.3)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EI_3btNjYidv"
   },
   "source": [
    "Demonstration on the streetview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0CaWxAhjYidv"
   },
   "outputs": [],
   "source": [
    "# Calculate the magnitude of the Gabor filter response given a kernel and an imput image\n",
    "def magnitude(image, kernel):\n",
    "    image = (image - image.mean()) / image.std() # Normalize images\n",
    "    return np.sqrt(ndi.convolve(image, np.real(kernel), mode='wrap')**2 +\n",
    "                   ndi.convolve(image, np.imag(kernel), mode='wrap')**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOZous_WYidv"
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def demoGabor2(frequency=(0.01,1,0.05), theta=(0,3.14,0.1), sigma=(0,5,0.1)):\n",
    "    plt.subplot(131)\n",
    "    plt.title('Original')\n",
    "    plt.imshow(image)\n",
    "    plt.subplot(132)\n",
    "    plt.title('Gabor kernel')\n",
    "    plt.imshow(np.real(gabor_kernel(frequency=frequency, theta=theta, sigma_x=sigma, sigma_y=sigma)), interpolation='nearest')\n",
    "    plt.subplot(133)\n",
    "    plt.title('Response magnitude')\n",
    "    plt.imshow(np.real(magnitude(image, gabor_kernel(frequency=frequency, theta=theta, sigma_x=sigma, sigma_y=sigma))), interpolation='nearest')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Z9XOB4SYidv"
   },
   "outputs": [],
   "source": [
    "demoGabor2(frequency=0.16, theta=1.4, sigma=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaO1xfOGYidv"
   },
   "source": [
    "### Filter banks\n",
    "- Different filters detect different edges, shapes,...\n",
    "- Not all seem useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aB41kT1GYidv"
   },
   "outputs": [],
   "source": [
    "# More images\n",
    "image3 = normalize_image(Xsm[3])\n",
    "image5 = normalize_image(Xsm[5])\n",
    "image13 = normalize_image(Xsm[13])\n",
    "\n",
    "image_names = ('3', '5', '8') # labels\n",
    "images = (image3, image5, image13)\n",
    "\n",
    "def plot_filter_bank(images):\n",
    "    # Create a set of kernels, apply them to each image, store the results\n",
    "    results = []\n",
    "    kernel_params = []\n",
    "    for theta in (0, 1):\n",
    "        theta = theta / 4. * np.pi\n",
    "        for frequency in (0.1, 0.2):\n",
    "            for sigma in (1, 3):\n",
    "                kernel = gabor_kernel(frequency, theta=theta,sigma_x=sigma,sigma_y=sigma)\n",
    "                params = 'theta=%.2f,\\nfrequency=%.2f\\nsigma=%.2f' % (theta, frequency, sigma)\n",
    "                kernel_params.append(params)\n",
    "                results.append((kernel, [magnitude(img, kernel) for img in images]))\n",
    "\n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(nrows=9, ncols=4, figsize=(6, 12))\n",
    "    plt.gray()\n",
    "    #fig.suptitle('Image responses for Gabor filter kernels', fontsize=12)\n",
    "    axes[0][0].axis('off')\n",
    "\n",
    "    # Plot original images\n",
    "    for label, img, ax in zip(image_names, images, axes[0][1:]):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(label, fontsize=9)\n",
    "        ax.axis('off')\n",
    "\n",
    "    for label, (kernel, magnitudes), ax_row in zip(kernel_params, results, axes[1:]):\n",
    "        # Plot Gabor kernel\n",
    "        ax = ax_row[0]\n",
    "        ax.imshow(np.real(kernel), interpolation='nearest') # Plot kernel\n",
    "        ax.set_ylabel(label, fontsize=7)\n",
    "        ax.set_xticks([]) # Remove axis ticks \n",
    "        ax.set_yticks([])\n",
    "\n",
    "        # Plot Gabor responses with the contrast normalized for each filter\n",
    "        vmin = np.min(magnitudes)\n",
    "        vmax = np.max(magnitudes)\n",
    "        for patch, ax in zip(magnitudes, ax_row[1:]):\n",
    "            ax.imshow(patch, vmin=vmin, vmax=vmax) # Plot convolutions\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.rcParams['figure.dpi'] = 80\n",
    "    plt.show()\n",
    "\n",
    "plot_filter_bank(images)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6R_hnllqYidw"
   },
   "source": [
    "Another example: Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4uZ_GPdYidw"
   },
   "outputs": [],
   "source": [
    "fmnist_data = oml.datasets.get_dataset(40996) # Download FMNIST data\n",
    "# Get the predictors X and the labels y\n",
    "X_fm, y_fm, c, a = fmnist_data.get_data(dataset_format='array', target=fmnist_data.default_target_attribute); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MD8_BRlYidw"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 200\n",
    "# build a list of figures for plotting\n",
    "def buildFigureList(fig, subfiglist, titles, length):\n",
    "    for i in range(0,length):\n",
    "        pixels = np.array(subfiglist[i], dtype='float')\n",
    "        pixels = pixels.reshape((28, 28))\n",
    "        a=fig.add_subplot(1,length,i+1)\n",
    "        imgplot =plt.imshow(pixels, cmap='gray_r')\n",
    "        a.set_title(titles[i], fontsize=6)\n",
    "        a.axes.get_xaxis().set_visible(False)\n",
    "        a.axes.get_yaxis().set_visible(False)\n",
    "    return\n",
    "\n",
    "subfiglist = []\n",
    "titles=[]\n",
    "\n",
    "for i in range(0,10):\n",
    "    subfiglist.append(X_fm[i])\n",
    "    titles.append(i)\n",
    "\n",
    "buildFigureList(plt.figure(1),subfiglist, titles, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYCwOZg8Yidw"
   },
   "source": [
    "Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "OxUpa4qRYidw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 100\n",
    "boot = X_fm[0].reshape((28, 28))\n",
    "image2=boot\n",
    "@interact\n",
    "def demoGabor3(frequency=(0.01,1,0.05), theta=(0,3.14,0.1), sigma=(0,5,0.1)):\n",
    "    plt.subplot(131)\n",
    "    plt.title('Original')\n",
    "    plt.imshow(image2)\n",
    "    plt.subplot(132)\n",
    "    plt.title('Gabor kernel')\n",
    "    plt.imshow(np.real(gabor_kernel(frequency=frequency, theta=theta, sigma_x=sigma, sigma_y=sigma)), interpolation='nearest')\n",
    "    plt.subplot(133)\n",
    "    plt.title('Response magnitude')\n",
    "    plt.imshow(np.real(magnitude(image2, gabor_kernel(frequency=frequency, theta=theta, sigma_x=sigma, sigma_y=sigma))), interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmkpI9kYYidw"
   },
   "outputs": [],
   "source": [
    "demoGabor3(frequency=0.81, theta=2.7, sigma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aq3-8lg4Yidw"
   },
   "source": [
    "Fashion MNIST with multiple filters (filter bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "GoFpSJJuYidw"
   },
   "outputs": [],
   "source": [
    "# Fetch some Fashion-MNIST images\n",
    "boot = X_fm[0].reshape(28, 28)\n",
    "shirt = X_fm[1].reshape(28, 28)\n",
    "dress = X_fm[2].reshape(28, 28)\n",
    "image_names = ('boot', 'shirt', 'dress')\n",
    "images = (boot, shirt, dress)\n",
    "\n",
    "plot_filter_bank(images)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qU2P04M8Yidw"
   },
   "source": [
    "## Convolutional neural nets\n",
    "* Finding relationships between individual pixels and the correct class is hard\n",
    "* We want to discover 'local' patterns (edges, lines, endpoints)\n",
    "* Representing such local patterns as features makes it easier to learn from them\n",
    "* We could use convolutions, but how to choose the filters?\n",
    "\n",
    "<img src=\"https://github.com/ML-course/master/blob/master/images/10_patches.png?raw=1\" alt=\"ml\" style=\"width: 300px;  margin-left: auto; margin-right: auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zsULjBQYidw"
   },
   "source": [
    "### Convolutional Neural Networks (ConvNets)\n",
    "* Instead on manually designing the filters, we can also _learn_ them based on data\n",
    "    * Choose filter sizes (manually), initialize with small random weights\n",
    "* Forward pass: Convolutional layer slides the filter over the input, generates the output\n",
    "* Backward pass: Update the filter weights according to the loss gradient\n",
    "* Illustration for 1 filter: \n",
    "\n",
    "<img src=\"https://github.com/ML-course/master/blob/master/images/cnn.png?raw=1\" alt=\"ml\" style=\"width: 500px;  margin-left: auto; margin-right: auto;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "id": "vac7H3HNYidw"
   },
   "source": [
    "### Convolutional layers: Feature maps\n",
    "* One filter is not sufficient to detect all relevant patterns in an image\n",
    "* A convolutional layer applies and learns $d$ filter in parallel  \n",
    "* Slide $d$ filters across the input image (in parallel) -> a (1x1xd) output per patch\n",
    "* Reassemble into a _feature map_ with $d$ 'channels', a (width x height x d) tensor.\n",
    "\n",
    "<img src=\"https://github.com/ML-course/master/blob/master/images/10_convolution.png?raw=1\" alt=\"ml\" style=\"width: 400px;  margin-left: auto; margin-right: auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_FWWcutYidw"
   },
   "source": [
    "### Border effects (zero padding)\n",
    "* Consider a 5x5 image and a 3x3 filter: there are only 9 possible locations, hence the output is a 3x3 feature map\n",
    "* If we want to maintain the image size, we use _zero-padding_, adding 0's all around the input tensor.\n",
    "\n",
    "<img src=\"https://github.com/ML-course/master/blob/master/images/10_padding.png?raw=1\" alt=\"ml\" style=\"float: left; width: 45%;\"/>\n",
    "<img src=\"https://github.com/ML-course/master/blob/master/images/10_padding_2.png?raw=1\" alt=\"ml\" style=\"float: left; width: 45%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYvkxOPAYidw"
   },
   "source": [
    "### Undersampling (striding)\n",
    "* Sometimes, we want to _downsample_ a high-resolution image\n",
    "    * Faster processing, less noisy (hence less overfitting)\n",
    "* One approach is to _skip_ values during the convolution\n",
    "    * Distance between 2 windows: _stride length_\n",
    "* Example with stride length 2 (without padding):\n",
    "    \n",
    "<img src=\"https://github.com/ML-course/master/blob/master/images/10_strides.png?raw=1\" alt=\"ml\" style=\"width: 500px;  margin-left: auto; margin-right: auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMLBW1wLYidx"
   },
   "source": [
    "### Max-pooling\n",
    "* Another approach to shrink the input tensors is _max-pooling_ :\n",
    "    - Run a filter with a fixed stride length over the image\n",
    "        - Usually 2x2 filters and stride lenght 2\n",
    "    - The filter simply returns the _max_ (or _avg_ ) of all values\n",
    "* Agressively reduces the number of weights (less overfitting)\n",
    "* Information from every input node spreads more quickly to output nodes\n",
    "    - In `pure` convnets, one input value spreads to 3x3 nodes of the first layer, 5x5 nodes of the second, etc.\n",
    "    - Without maxpooling, you need much deeper networks, harder to train\n",
    "* Increases _translation invariance_ : patterns can affect the predictions no matter where they occur in the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWkWdA1ZYidx"
   },
   "source": [
    "## Convolutional nets in practice\n",
    "* ConvNets usually use multiple convolutional layers to learn patterns at different levels of abstraction\n",
    "    * Find local patterns first (e.g. edges), then patterns across those patterns\n",
    "* Use MaxPooling layers to reduce resolution, increase translation invariance\n",
    "* Use sufficient filters in the first layer (otherwise information gets lost)\n",
    "* In deeper layers, use increasingly more filters\n",
    "    * Preserve information about the input as resolution descreases\n",
    "    * Avoid decreasing the number of activations (resolution x nr of filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBJasfAZYidx"
   },
   "source": [
    "Example with Keras:\n",
    "    \n",
    "* `Conv2D` for 2D convolutional layers\n",
    "    - 32 filters (default), randomly initialized (from uniform distribution)\n",
    "    - Deeper layers use 64 filters\n",
    "    - Filter size is 3x3\n",
    "    - ReLU activation to simplify training of deeper networks\n",
    "* `MaxPooling2D` for max-pooling\n",
    "    - 2x2 pooling reduces the number of inputs by a factor 4\n",
    "        \n",
    "``` python\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', \n",
    "                        input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyVG69bkYidx"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dsycp1XqYidx"
   },
   "source": [
    "Observe how the input image on 28x28x1 is transformed to a 3x3x64 feature map  \n",
    "* Convolutional layer:\n",
    "    * No zero-padding: every output 2 pixels less in every dimension\n",
    "    * 320 weights: (3x3 filter weights + 1 bias) * 32 filters\n",
    "* After every MaxPooling, resolution halved in every dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "id": "PhtFQwdjYidx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U33_pUgfYidx"
   },
   "source": [
    "Completing the network\n",
    "\n",
    "* To classify the images, we still need a Dense and Softmax layer.\n",
    "* We need to flatten the 3x3x64 feature map to a vector of size 576\n",
    "\n",
    "``` python\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYKm2llNYidx"
   },
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFDVrKuQYidx"
   },
   "source": [
    "Complete network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "id": "vBwH9sGEYidx"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esBEzxeBYidx"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66ch90fUYidx"
   },
   "source": [
    "Run the model on MNIST dataset\n",
    "* Train and test as usual (takes about 5 minutes): 99% accuracy\n",
    "    * Compared to 97,8% accuracy with the dense architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "id": "JG_cfcAnYidx"
   },
   "source": [
    "###### Model was trained beforehand and saved. Uncomment if you want to run from scratch\n",
    "``` python\n",
    "import pickle\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_images, train_labels, epochs=5, batch_size=64, verbose=0,\n",
    "                    validation_data=(test_images, test_labels))\n",
    "\n",
    "model.save(os.path.join(model_dir, 'cats_and_dogs_small_0.h5'))\n",
    "with open(os.path.join(model_dir, 'cats_and_dogs_small_0_history.p'), 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQaIIUBFYidx"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(os.path.join(model_dir, 'cats_and_dogs_small_0.h5'))\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(\"Accuracy: \", test_acc)\n",
    "\n",
    "history = pickle.load(open(\"../data/models/cats_and_dogs_small_0_history.p\", \"rb\"))\n",
    "pd.DataFrame(history).plot(lw=2,style=['b:','r:','b-','r-']);\n",
    "plt.xlabel('epochs');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sU7zXO7mYidx"
   },
   "source": [
    "Tip:\n",
    "* Training ConvNets can take a lot of time\n",
    "* Save the trained model (and history) to disk so that you can reload it later\n",
    "\n",
    "``` python\n",
    "model.save(os.path.join(model_dir, 'cats_and_dogs_small.h5'))\n",
    "with open(os.path.join(model_dir, 'cats_and_dogs_small_history.p'), 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LG9VevMHYidx"
   },
   "source": [
    "## Cats vs Dogs\n",
    "* A more realistic dataset: [Cats vs Dogs](https://www.kaggle.com/c/dogs-vs-cats/data)\n",
    "    - Colored JPEG images, different sizes\n",
    "    - Not nicely centered, translation invariance is important\n",
    "* Preprocessing\n",
    "    - Create balanced subsample of 4000 colored images \n",
    "        - 2000 for training, 1000 validation, 1000 testing\n",
    "    - Decode JPEG images to floating-point tensors\n",
    "    - Rescale pixel values to [0,1]\n",
    "    - Resize images to 150x150 pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmTTvevaYidy"
   },
   "source": [
    "###### Data is already prepared in the 'data' folder, you don't need to run this\n",
    "###### Uncomment if you want to run from scratch\n",
    "\n",
    "```python\n",
    "import os, shutil \n",
    "# Download data from https://www.kaggle.com/c/dogs-vs-cats/data\n",
    "# Uncompress `train.zip` into the `original_dataset_dir`\n",
    "original_dataset_dir = '../data/cats-vs-dogs_original'\n",
    "\n",
    "# The directory where we will\n",
    "# store our smaller dataset\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "if not os.path.exists(base_dir):\n",
    "    os.mkdir(base_dir)\n",
    "    os.mkdir(train_dir)\n",
    "    os.mkdir(validation_dir)\n",
    "    os.mkdir(test_dir)\n",
    "    \n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "\n",
    "if not os.path.exists(train_cats_dir):\n",
    "    os.mkdir(train_cats_dir)\n",
    "    os.mkdir(train_dogs_dir)\n",
    "    os.mkdir(validation_cats_dir)\n",
    "    os.mkdir(validation_dogs_dir)\n",
    "    os.mkdir(test_cats_dir)\n",
    "    os.mkdir(test_dogs_dir)\n",
    "\n",
    "# Copy first 1000 cat images to train_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copy next 500 cat images to validation_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 cat images to test_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy first 1000 dog images to train_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to validation_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to test_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxG3qHwZYidy"
   },
   "source": [
    "### Data generators\n",
    "* `ImageDataGenerator`: allows to encode, resize, and rescale JPEG images\n",
    "* Returns a Python _generator_ we can endlessly query for batches of images\n",
    "* Separately for training, validation, and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wh63QTpdYidy"
   },
   "source": [
    "``` python\n",
    "train_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        train_dir, # Directory with images\n",
    "        target_size=(150, 150), # Resize images \n",
    "        batch_size=20, # Return 20 images at a time\n",
    "        class_mode='binary') # Binary labels\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Gtmt7WTYidy"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qruSI24RYidy"
   },
   "outputs": [],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    plt.figure(figsize=(10,5))\n",
    "    for i in range(7):\n",
    "        plt.subplot(171+i)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        imgplot = plt.imshow(data_batch[i])\n",
    "        plt.title('cat' if labels_batch[i] == 0 else 'dog')\n",
    "        plt.tight_layout()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwyrCAm0Yidy"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzNmmf-oYidy"
   },
   "source": [
    "Since the images are larger and more complex, we add another convolutional layer and increase the number of filters to 128.\n",
    "\n",
    "``` python\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "id": "C2VPJ0GGYidy"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztXKZpfXYidy"
   },
   "source": [
    "### Training\n",
    "* The `fit` function also supports generators\n",
    "    - 100 steps per epoch (batch size: 20 images per step), for 30 epochs\n",
    "    - Provide a separate generator for the validation data\n",
    "    \n",
    "``` python\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])\n",
    "history = model.fit(\n",
    "      train_generator, steps_per_epoch=100,\n",
    "      epochs=30, verbose=0,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8mxwm43Yidy"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mw_fQHihYidy"
   },
   "source": [
    "###### Uncomment to run. Training takes about 1 hours on CPU\n",
    "###### We save the trained model (and history) to disk so that we can reload it later\n",
    "###### Note: TensorFlow 2.1 seems to have a bug resulting in an error message 'sample_weight modes were coerced'\n",
    "``` python\n",
    "import pickle\n",
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=30, verbose=1,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)\n",
    "model.save(os.path.join(model_dir, 'cats_and_dogs_small_1.h5'))\n",
    "with open(os.path.join(model_dir, 'cats_and_dogs_small_1_history.p'), 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jt2dfLfYidy"
   },
   "source": [
    "#### Results\n",
    "* The network seems to be overfitting. Validation accuracy is stuck at 75% while the training accuracy reaches 100%\n",
    "* There are many things we can do:\n",
    "    - Regularization (e.g. Dropout, L1/L2, Batch Normalization,...)\n",
    "    - Generating more training data\n",
    "    - Meta-learning: Use pretrained rather than randomly initialized filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "OYxkgszVYidy"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "history = pickle.load(open(\"../data/models/cats_and_dogs_small_1_history.p\", \"rb\"))\n",
    "pd.DataFrame(history).plot(lw=2,style=['b:','r:','b-','r-']);\n",
    "plt.xlabel('epochs');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaFft7RyYidy"
   },
   "source": [
    "### Data augmentation\n",
    "* Generate new images via image transformations\n",
    "    - Images will be randomly transformed _every epoch_\n",
    "* We can again use a data generator to do this\n",
    "\n",
    "``` python\n",
    "datagen = ImageDataGenerator(\n",
    "      rotation_range=40,     # Rotate image up to 40 degrees\n",
    "      width_shift_range=0.2, # Shift image left-right up to 20% of image width\n",
    "      height_shift_range=0.2,# Shift image up-down up to 20% of image height\n",
    "      shear_range=0.2,       # Shear (slant) the image up to 0.2 degrees\n",
    "      zoom_range=0.2,        # Zoom in up to 20%\n",
    "      horizontal_flip=True,  # Horizontally flip the image\n",
    "      fill_mode='nearest')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pmYGo1pHYidy"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jL4NlLsnYidy"
   },
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "NEfsj5Y8Yidy"
   },
   "outputs": [],
   "source": [
    "# This is module with image preprocessing utilities\n",
    "from tensorflow.keras.preprocessing import image\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "\n",
    "train_cats_dir = os.path.join(base_dir, 'train', 'cats')\n",
    "fnames = [os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)]\n",
    "\n",
    "# We pick one image to \"augment\"\n",
    "img_path = fnames[5]\n",
    "\n",
    "# Read the image and resize it\n",
    "img = image.load_img(img_path, target_size=(150, 150))\n",
    "\n",
    "# Convert it to a Numpy array with shape (150, 150, 3)\n",
    "x = image.img_to_array(img)\n",
    "\n",
    "# Reshape it to (1, 150, 150, 3)\n",
    "x = x.reshape((1,) + x.shape)\n",
    "\n",
    "# The .flow() command below generates batches of randomly transformed images.\n",
    "# It will loop indefinitely, so we need to `break` the loop at some point!\n",
    "for a in range(2):\n",
    "    i = 0\n",
    "    for batch in datagen.flow(x, batch_size=1):\n",
    "        plt.subplot(141+i) \n",
    "        plt.xticks([]) \n",
    "        plt.yticks([])\n",
    "        imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
    "        i += 1\n",
    "        if i % 4 == 0:\n",
    "            break\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEciXicRYidz"
   },
   "source": [
    "We also add Dropout before the Dense layer\n",
    "\n",
    "``` python\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VvnfCeBoYidz"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZlqDWISyYidz"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,)\n",
    "\n",
    "# Note that the validation data should not be augmented!\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hucva5TxYidz"
   },
   "source": [
    "###### Training takes about 6 hours on CPU, 30 min on 1 GPU\n",
    "###### Uncomment to run\n",
    "\n",
    "``` python\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=60, # 2000 images / batch size 32\n",
    "      epochs=100,  verbose=0,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=30) # About 1000/32\n",
    "model.save(os.path.join(model_dir, 'cats_and_dogs_small_2.h5'))\n",
    "with open(os.path.join(model_dir, 'cats_and_dogs_small_2_history.p'), 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7Vu-dtOYidz"
   },
   "source": [
    "(Almost) no more overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_8mOXUUYidz"
   },
   "outputs": [],
   "source": [
    "history = pickle.load(open(\"../data/models/cats_and_dogs_small_2_history.p\", \"rb\"))\n",
    "pd.DataFrame(history).plot(lw=2,style=['b:','r:','b-','r-']);\n",
    "plt.xlabel('epochs');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1r71cd1Yidz"
   },
   "source": [
    "## Interpreting the model\n",
    "* Let's see what the convnet is learning exactly by observing the intermediate feature maps\n",
    "    - A layer's output is also called its _activation_\n",
    "* We can choose a specific test image, and observe the outputs\n",
    "* We can retrieve and visualize the activation for every filter for every layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOHuCw-qYidz"
   },
   "source": [
    "* Layer 0: has activations of resolution 148x148 for each of its 32 filters\n",
    "* Layer 2: has activations of resolution 72x72 for each of its 64 filters\n",
    "* Layer 4: has activations of resolution 34x34 for each of its 128 filters\n",
    "* Layer 6: has activations of resolution 15x15 for each of its 128 filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzP8S1nlYidz"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(os.path.join(model_dir, 'cats_and_dogs_small_2.h5'))\n",
    "model.summary()  # As a reminder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "K7SSXQtOYidz"
   },
   "outputs": [],
   "source": [
    "img_path = os.path.join(base_dir, 'test/cats/cat.1700.jpg')\n",
    "\n",
    "# We preprocess the image into a 4D tensor\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "img = image.load_img(img_path, target_size=(150, 150))\n",
    "img_tensor = image.img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0) \n",
    "# Remember that the model was trained on inputs\n",
    "# that were preprocessed in the following way:\n",
    "img_tensor /= 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27RQnLNhYidz"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "\n",
    "# Extracts the outputs of the top 8 layers:\n",
    "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
    "# Creates a model that will return these outputs, given the model input:\n",
    "activation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "# This will return a list of 5 Numpy arrays:\n",
    "# one array per layer activation\n",
    "activations = activation_model.predict(img_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwiT9WniYidz"
   },
   "source": [
    "* To extract the activations, we create a new model that outputs the trained layers\n",
    "    * 8 output layers in total (only the convolutional part)\n",
    "* We input a test image for prediction and then read the relevant outputs\n",
    "\n",
    "``` python\n",
    "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
    "activation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "activations = activation_model.predict(img_tensor)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgdCkSW3Yidz"
   },
   "source": [
    "Output of the first Conv2D layer, 3rd channel (filter):\n",
    "* Similar to a diagonal edge detector\n",
    "* Your own channels may look different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "K1UNI8TkYidz"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 120\n",
    "first_layer_activation = activations[0]\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "ax1.imshow(img_tensor[0])\n",
    "ax2.matshow(first_layer_activation[0, :, :, 2], cmap='viridis')\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "ax1.set_xlabel('Input image')\n",
    "ax2.set_xlabel('Activation of filter 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfO-V-9hYidz"
   },
   "source": [
    "Output of filter 16:\n",
    "* Cat eye detector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "qIQFfZEoYidz"
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "ax1.imshow(img_tensor[0])\n",
    "ax2.matshow(first_layer_activation[0, :, :,16], cmap='viridis')\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "ax1.set_xlabel('Input image')\n",
    "ax2.set_xlabel('Activation of filter 16');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnfFIOe4Yidz"
   },
   "source": [
    "The same filter responds quite differently for other inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rge-b3xYidz"
   },
   "outputs": [],
   "source": [
    "img_path = os.path.join(base_dir, 'test/dogs/dog.1528.jpg')\n",
    "\n",
    "# We preprocess the image into a 4D tensor\n",
    "img = image.load_img(img_path, target_size=(150, 150))\n",
    "img_tensor2 = image.img_to_array(img)\n",
    "img_tensor2 = np.expand_dims(img_tensor2, axis=0) \n",
    "# Remember that the model was trained on inputs\n",
    "# that were preprocessed in the following way:\n",
    "img_tensor2 /= 255.\n",
    "\n",
    "activations2 = activation_model.predict(img_tensor2)\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "first_layer_activation2 = activations2[0]\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "ax1.imshow(img_tensor2[0])\n",
    "ax2.matshow(first_layer_activation2[0, :, :, 16], cmap='viridis')\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "ax1.set_xlabel('Input image')\n",
    "ax2.set_xlabel('Activation of filter 16');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOrFvcNaYid0"
   },
   "outputs": [],
   "source": [
    "images_per_row = 16\n",
    "\n",
    "layer_names = []\n",
    "for layer in model.layers[:8]:\n",
    "    layer_names.append(layer.name)\n",
    "\n",
    "def plot_activations(layer_index, activations):\n",
    "    start = layer_index\n",
    "    end = layer_index+1\n",
    "    # Now let's display our feature maps\n",
    "    for layer_name, layer_activation in zip(layer_names[start:end], activations[start:end]):\n",
    "        # This is the number of features in the feature map\n",
    "        n_features = layer_activation.shape[-1]\n",
    "\n",
    "        # The feature map has shape (1, size, size, n_features)\n",
    "        size = layer_activation.shape[1]\n",
    "\n",
    "        # We will tile the activation channels in this matrix\n",
    "        n_cols = n_features // images_per_row\n",
    "        display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "\n",
    "        # We'll tile each filter into this big horizontal grid\n",
    "        for col in range(n_cols):\n",
    "            for row in range(images_per_row):\n",
    "                channel_image = layer_activation[0,\n",
    "                                                 :, :,\n",
    "                                                 col * images_per_row + row]\n",
    "                # Post-process the feature to make it visually palatable\n",
    "                channel_image -= channel_image.mean()\n",
    "                channel_image /= channel_image.std()\n",
    "                channel_image *= 64\n",
    "                channel_image += 128\n",
    "                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "                display_grid[col * size : (col + 1) * size,\n",
    "                             row * size : (row + 1) * size] = channel_image\n",
    "\n",
    "        # Display the grid\n",
    "        scale = 1. / size\n",
    "        plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                            scale * display_grid.shape[0]))\n",
    "        plt.title(\"Activation of layer {} ({})\".format(layer_index+1,layer_name))\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5l_5tKAYid0"
   },
   "source": [
    "* First 2 convolutional layers: various edge detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "KRMhlJq9Yid0"
   },
   "outputs": [],
   "source": [
    "plot_activations(0, activations)\n",
    "plot_activations(2, activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNcHZCb5Yid0"
   },
   "source": [
    "* 3rd convolutional layer: increasingly abstract: ears, eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "-TLo8KGCYid0"
   },
   "outputs": [],
   "source": [
    "plot_activations(4, activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "id": "nKTu3_vEYid0"
   },
   "source": [
    "* Last convolutional layer: more abstract patterns\n",
    "* Empty filter activations: input image does not have the information that the filter was interested in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "mvywAcb-Yid0"
   },
   "outputs": [],
   "source": [
    "plot_activations(6, activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqKCdzHFYid0"
   },
   "source": [
    "* Same layer, with dog image input\n",
    "    * Very different activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWHbeZdvYid0"
   },
   "outputs": [],
   "source": [
    "plot_activations(6, activations2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dt2x7EodYid0"
   },
   "source": [
    "### Spatial hierarchies\n",
    "* Deep convnets can learn _spatial hierarchies_ of patterns\n",
    "    - First layer can learn very local patterns (e.g. edges)\n",
    "    - Second layer can learn specific combinations of patterns\n",
    "    - Every layer can learn increasingly complex _abstractions_\n",
    "    \n",
    "<img src=\"https://github.com/ML-course/master/blob/master/images/10_hierarchy.png?raw=1\" alt=\"ml\" style=\"width: 500px;  margin-left: auto; margin-right: auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34QNNpdxYid0"
   },
   "source": [
    "### Visualizing the learned filters\n",
    "* The filters themselves can be visualized by finding the input image that they are maximally responsive to\n",
    "* _gradient ascent in input space_ : start from a random image, use loss to update the pixel values to values that the filter responds to more strongly\n",
    "\n",
    "``` python\n",
    "    from keras import backend as K\n",
    "    input_img = np.random.random((1, size, size, 3)) * 20 + 128.\n",
    "    loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "    grads = K.gradients(loss, model.input)[0] # Compute gradient\n",
    "    for i in range(40): # Run gradient ascent for 40 steps\n",
    "        loss_v, grads_v = K.function([input_img], [loss, grads])\n",
    "        input_img_data += grads_v * step\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b89ufvY9Yid0"
   },
   "outputs": [],
   "source": [
    "#tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()\n",
    "\n",
    "# Convert tensor to image\n",
    "def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def generate_pattern(layer_name, filter_index, size=150):\n",
    "    # Build a loss function that maximizes the activation\n",
    "    # of the nth filter of the layer considered.\n",
    "    layer_output = model.get_layer(layer_name).output\n",
    "    loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "    # Compute the gradient of the input picture wrt this loss\n",
    "    grads = K.gradients(loss, model.input)[0]\n",
    "\n",
    "    # Normalization trick: we normalize the gradient\n",
    "    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
    "\n",
    "    # This function returns the loss and grads given the input picture\n",
    "    iterate = K.function([model.input], [loss, grads])\n",
    "    \n",
    "    # We start from a gray image with some noise\n",
    "    input_img_data = np.random.random((1, size, size, 3)) * 20 + 128.\n",
    "\n",
    "    # Run gradient ascent for 40 steps\n",
    "    step = 1.\n",
    "    for i in range(40):\n",
    "        loss_value, grads_value = iterate([input_img_data])\n",
    "        input_img_data += grads_value * step\n",
    "        \n",
    "    img = input_img_data[0]\n",
    "    return deprocess_image(img)\n",
    "\n",
    "def visualize_filter(layer_name):\n",
    "    size = 64\n",
    "    margin = 5\n",
    "\n",
    "    # This a empty (black) image where we will store our results.\n",
    "    results = np.zeros((8 * size + 7 * margin, 8 * size + 7 * margin, 3))\n",
    "\n",
    "    for i in range(8):  # iterate over the rows of our results grid\n",
    "        for j in range(8):  # iterate over the columns of our results grid\n",
    "            # Generate the pattern for filter `i + (j * 8)` in `layer_name`\n",
    "            filter_img = generate_pattern(layer_name, i + (j * 8), size=size)\n",
    "\n",
    "            # Put the result in the square `(i, j)` of the results grid\n",
    "            horizontal_start = i * size + i * margin\n",
    "            horizontal_end = horizontal_start + size\n",
    "            vertical_start = j * size + j * margin\n",
    "            vertical_end = vertical_start + size\n",
    "            results[horizontal_start: horizontal_end, vertical_start: vertical_end, :] = filter_img\n",
    "\n",
    "    # Display the results grid\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow((results * 255).astype(np.uint8))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRLEv-kEYid0"
   },
   "source": [
    "* Learned filters of second convolutional layer\n",
    "* Mostly general, some respond to specific shapes/colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4Q-5B0BYid0"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 60\n",
    "visualize_filter('conv2d_9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqfTuWlHYid0"
   },
   "source": [
    "* Learned filters of last convolutional layer\n",
    "* More focused on center, some vague cat/dog head shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4PfAZhVkYid0"
   },
   "outputs": [],
   "source": [
    "visualize_filter('conv2d_11')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abJz4IaqYid0"
   },
   "source": [
    "Let's do this again for the `VGG16` network pretrained on `ImageNet` (much larger)\n",
    "    \n",
    "``` python\n",
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s714tCKwYid0"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "model = VGG16(weights='imagenet',\n",
    "              include_top=False)\n",
    "\n",
    "layer_name = 'block3_conv1'\n",
    "filter_index = 0\n",
    "\n",
    "layer_output = model.get_layer(layer_name).output\n",
    "loss = K.mean(layer_output[:, :, :, filter_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "phKGGuICYid0"
   },
   "outputs": [],
   "source": [
    "# VGG16 model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ifi69_O5Yid1"
   },
   "source": [
    "* Visualize convolution filters 0-2 from layer 5 of the VGG network trained on ImageNet\n",
    "* Some respond to dots or waves in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "hCDhlspkYid1"
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    plt.subplot(131+i) \n",
    "    plt.xticks([]) \n",
    "    plt.yticks([])\n",
    "    plt.imshow(generate_pattern('block3_conv1', i))\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MskDLTfYid1"
   },
   "source": [
    "First 64 filters for 1st convolutional layer in block 1: simple edges and colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "id": "D6kIhu9FYid1"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 60\n",
    "visualize_filter('block1_conv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdnaUk77Yid1"
   },
   "source": [
    "Filters in 2nd block of convolution layers: simple textures (combined edges and colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "gWI98oJsYid1"
   },
   "outputs": [],
   "source": [
    "visualize_filter('block2_conv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KokQWBeYid1"
   },
   "source": [
    "Filters in 3rd block of convolution layers: more natural textures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "ObREw1OWYid1"
   },
   "outputs": [],
   "source": [
    "visualize_filter('block3_conv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuqpCfZoYid1"
   },
   "source": [
    "Filters in 4th block of convolution layers: feathers, eyes, leaves,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "id": "9a7E3A5ZYid1"
   },
   "outputs": [],
   "source": [
    "visualize_filter('block4_conv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Co-4wIgXYid1"
   },
   "source": [
    "## Visualizing class activation\n",
    "* We can also visualize which part of the input image had the greatest influence on the final classification\n",
    "    - Helpful for interpreting what the model is paying attention to\n",
    "* _Class activation maps_ : produce heatmap over the input image\n",
    "    - Take the output feature map of a convolution layer (e.g. the last one)\n",
    "    - Weigh every filter by the gradient of the class with respect to the filter\n",
    "\n",
    "<img src=\"https://github.com/ML-course/master/blob/master/images/activation_map.png?raw=1\" alt=\"ml\" style=\"width: 600px;  margin-left: auto; margin-right: auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32NVbXc3Yid1"
   },
   "source": [
    "Illustration (cats vs dogs)\n",
    "* These were the output feature maps of the last convolutional layer\n",
    "    * These are flattened and fed to the dense layer\n",
    "* Compute gradient of the 'cat' node output wrt. every filter output (pixel) here\n",
    "    * Average the gradients per filter, use that as the filter weight\n",
    "* Take the weighted sum of all filter maps to get the class activation map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Awof-SheYid1"
   },
   "outputs": [],
   "source": [
    "plot_activations(6, activations2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gim5T8cWYid1"
   },
   "source": [
    "More realistic example:\n",
    "* Try VGG (including the dense layers) and an image from ImageNet\n",
    "``` python\n",
    "model = VGG16(weights='imagenet')\n",
    "```\n",
    "<img src=\"https://github.com/ML-course/master/blob/master/images/10_elephants.jpg?raw=1\" alt=\"ml\" style=\"float: left; width: 75%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIlP7l-EYid1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "K.clear_session()\n",
    "# Note that we are including the densely-connected classifier on top;\n",
    "# all previous times, we were discarding it.\n",
    "model = VGG16(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ksmlxcosYid1"
   },
   "outputs": [],
   "source": [
    "## From Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "# The local path to our target image\n",
    "img_path = '../images/10_elephants.jpg'\n",
    "\n",
    "# `img` is a PIL image of size 224x224\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "# `x` is a float32 Numpy array of shape (224, 224, 3)\n",
    "x = image.img_to_array(img)\n",
    "\n",
    "# We add a dimension to transform our array into a \"batch\"\n",
    "# of size (1, 224, 224, 3)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "# Finally we preprocess the batch\n",
    "# (this does channel-wise color normalization)\n",
    "x = preprocess_input(x)# This is the \"african elephant\" entry in the prediction vector\n",
    "african_elephant_output = model.output[:, 386]\n",
    "\n",
    "# The is the output feature map of the `block5_conv3` layer,\n",
    "# the last convolutional layer in VGG16\n",
    "last_conv_layer = model.get_layer('block5_conv3')\n",
    "\n",
    "# This is the gradient of the \"african elephant\" class with regard to\n",
    "# the output feature map of `block5_conv3`\n",
    "grads = K.gradients(african_elephant_output, last_conv_layer.output)[0]\n",
    "\n",
    "# This is a vector of shape (512,), where each entry\n",
    "# is the mean intensity of the gradient over a specific feature map channel\n",
    "pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "# This function allows us to access the values of the quantities we just defined:\n",
    "# `pooled_grads` and the output feature map of `block5_conv3`,\n",
    "# given a sample image\n",
    "iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n",
    "\n",
    "# These are the values of these two quantities, as Numpy arrays,\n",
    "# given our sample image of two elephants\n",
    "pooled_grads_value, conv_layer_output_value = iterate([x])\n",
    "\n",
    "# We multiply each channel in the feature map array\n",
    "# by \"how important this channel is\" with regard to the elephant class\n",
    "for i in range(512):\n",
    "    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
    "\n",
    "# The channel-wise mean of the resulting feature map\n",
    "# is our heatmap of class activation\n",
    "\n",
    "heatmap = np.mean(conv_layer_output_value, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4Qv9fPDYid1"
   },
   "source": [
    "Preprocessing  \n",
    "* Load image\n",
    "* Resize to 224 x 224 (what VGG was trained on)\n",
    "* Do the same preprocessing (Keras VGG utility)\n",
    "\n",
    "``` python\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "img_path = '../images/10_elephants.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0) # Transform to batch of size (1, 224, 224, 3)\n",
    "x = preprocess_input(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpwMqxzCYid1"
   },
   "source": [
    "* Sanity test: do we get the right prediction?\n",
    "    \n",
    "``` python\n",
    "preds = model.predict(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "BKpsr0cmYid1"
   },
   "outputs": [],
   "source": [
    "preds = model.predict(x)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBKx3FVVYid1"
   },
   "source": [
    "Visualize the class activation map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "d3-Y6dbaYid2"
   },
   "outputs": [],
   "source": [
    "heatmap = np.maximum(heatmap, 0)\n",
    "heatmap /= np.max(heatmap)\n",
    "plt.matshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g51dpRMXYid2"
   },
   "outputs": [],
   "source": [
    "# pip install opencv-python\n",
    "import cv2\n",
    "\n",
    "# We use cv2 to load the original image\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "# We resize the heatmap to have the same size as the original image\n",
    "heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "\n",
    "# We convert the heatmap to RGB\n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "# We apply the heatmap to the original image\n",
    "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "# 0.4 here is a heatmap intensity factor\n",
    "superimposed_img = heatmap * 0.4 + img\n",
    "\n",
    "# Save the image to disk\n",
    "cv2.imwrite('../images/elephant_cam.jpg', superimposed_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94L2QIFtYid2"
   },
   "source": [
    "Superimposed on the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGos5KvGYid2"
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('../images/elephant_cam.jpg')\n",
    "RGB_im = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.imshow(RGB_im)\n",
    "plt.title('Class activation map')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bs5a5voYid2"
   },
   "source": [
    "## Using pretrained networks\n",
    "* We can re-use pretrained networks instead of training from scratch\n",
    "* Learned features can be a generic model of the visual world\n",
    "* Use _convolutional base_ to contruct features, then train any classifier on new data\n",
    "* Also called _transfer learning_ , which is a kind of _meta-learning_\n",
    "\n",
    "<img src=\"https://github.com/ML-course/master/blob/master/images/10_pretraining.png?raw=1\" alt=\"ml\" style=\"width: 600px;  margin-left: auto; margin-right: auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZo8TeQCYid2"
   },
   "source": [
    "* Let's instantiate the VGG16 model (without the dense layers)\n",
    "* Final feature map has shape (4, 4, 512)\n",
    "``` python\n",
    "conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WyqK5BUdYid2"
   },
   "outputs": [],
   "source": [
    "conv_base = VGG16(weights='imagenet', \n",
    "                  include_top=False,\n",
    "                  input_shape=(150, 150, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "mrMQE7ccYid2"
   },
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irN1lLgHYid2"
   },
   "source": [
    "## Using pre-trained networks: 3 ways\n",
    "* Fast feature extraction (similar task, little data)\n",
    "    - Call `predict` from the convolutional base to build new features\n",
    "    - Use outputs as input to a new neural net (or other algorithm)\n",
    "* End-to-end tuning (similar task, lots of data + data augmentation)\n",
    "    - Extend the convolutional base model with a new dense layer\n",
    "    - Train it end to end on the new data (expensive!)\n",
    "* Fine-tuning (somewhat different task)\n",
    "    - Unfreeze a few of the top convolutional layers, and retrain\n",
    "        - Update only the more abstract representations\n",
    "    \n",
    "<img src=\"https://github.com/ML-course/master/blob/master/images/pretraining.png?raw=1\" alt=\"ml\" style=\"width: 700px;  margin-left: auto; margin-right: auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bWNYRaoYid2"
   },
   "source": [
    "### Fast feature extraction (without data augmentation)\n",
    "* Run every batch through the pre-trained convolutional base\n",
    "    \n",
    "``` python\n",
    "generator = datagen.flow_from_directory(dir, target_size=(150, 150),\n",
    "        batch_size=batch_size, class_mode='binary')\n",
    "for inputs_batch, labels_batch in generator:\n",
    "    features_batch = conv_base.predict(inputs_batch)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKQGgK1pYid2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 20\n",
    "\n",
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
    "    labels = np.zeros(shape=(sample_count))\n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            # Note that since generators yield data indefinitely in a loop,\n",
    "            # we must `break` after every image has been seen once.\n",
    "            break\n",
    "    return features, labels\n",
    "\n",
    "train_features, train_labels = extract_features(train_dir, 2000)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 1000)\n",
    "test_features, test_labels = extract_features(test_dir, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQSwKXJ8Yid2"
   },
   "outputs": [],
   "source": [
    "train_features = np.reshape(train_features, (2000, 4 * 4 * 512))\n",
    "validation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))\n",
    "test_features = np.reshape(test_features, (1000, 4 * 4 * 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCrK2MPEYid2"
   },
   "source": [
    "* Build Dense neural net (with Dropout)\n",
    "* Train and evaluate with the transformed examples\n",
    "\n",
    "``` python\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdEEhWfJYid2"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs=30, verbose=0,\n",
    "                    batch_size=20,\n",
    "                    validation_data=(validation_features, validation_labels))\n",
    "\n",
    "model.save(os.path.join(model_dir, 'cats_and_dogs_small_3a.h5'))\n",
    "with open(os.path.join(model_dir, 'cats_and_dogs_small_3a_history.p'), 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZ8xePwVYid2"
   },
   "source": [
    "* Validation accuracy around 90%, much better!\n",
    "* Still overfitting, despite the Dropout: not enough training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "c12x7gspYid2"
   },
   "outputs": [],
   "source": [
    "history = pickle.load(open(\"../data/models/cats_and_dogs_small_3a_history.p\", \"rb\"))\n",
    "print(\"Max val_acc\",np.max(history['val_acc']))\n",
    "pd.DataFrame(history).plot(lw=2,style=['b:','r:','b-','r-']);\n",
    "plt.xlabel('epochs');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkhlyKiJYid2"
   },
   "source": [
    "### Fast feature extraction (with data augmentation)\n",
    "* Simply add the Dense layers to the convolutional base\n",
    "* _Freeze_ the convolutional base (before you compile)\n",
    "    * Without freezing, you train it end-to-end (expensive)\n",
    "\n",
    "``` python\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "conv_base.trainable = False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJ6M3rADYid2"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "I2IDjj4HYid2"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bvj-NXd-Yid3"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "# Note that the validation data should not be augmented!\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgW_hOBqYid3"
   },
   "source": [
    "##### Takes a long time. Uncomment if you really want to run it\n",
    "``` python\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50,\n",
    "      verbose=0)\n",
    "model.save(os.path.join(model_dir, 'cats_and_dogs_small_3b.h5'))\n",
    "with open(os.path.join(model_dir, 'cats_and_dogs_small_3b_history.p'), 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64lejanqYid3"
   },
   "source": [
    "Data augmentation and training (takes a LONG time) \n",
    "``` python\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255, rotation_range=40, width_shift_range=0.2,\n",
    "      height_shift_range=0.2, shear_range=0.2, zoom_range=0.2,\n",
    "      horizontal_flip=True, fill_mode='nearest')\n",
    "train_generator = train_datagen.flow_from_directory(dir,\n",
    "      target_size=(150, 150), batch_size=20, class_mode='binary')\n",
    "history = model.fit_generator(\n",
    "      train_generator, steps_per_epoch=100, epochs=30,\n",
    "      validation_data=validation_generator, validation_steps=50)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSyMspu4Yid3"
   },
   "source": [
    "We now get about 90% accuracy again, and very little overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEv2sosAYid3"
   },
   "outputs": [],
   "source": [
    "history = pickle.load(open(\"../data/models/cats_and_dogs_small_3b_history.p\", \"rb\"))\n",
    "print(\"Max val_acc\",np.max(history['val_acc']))\n",
    "pd.DataFrame(history).plot(lw=2,style=['b:','r:','b-','r-']);\n",
    "plt.xlabel('epochs');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFTInQ94Yid3"
   },
   "source": [
    "### Fine-tuning\n",
    "* Add your custom network on top of an already trained base network.\n",
    "* Freeze the base network, but unfreeze the last block of conv layers.\n",
    "\n",
    "``` python\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFTqNFfIYid3"
   },
   "source": [
    "Visualized\n",
    "\n",
    "<img src=\"https://github.com/ML-course/master/blob/master/images/10_vgg16_fine_tuning.png?raw=1\" alt=\"ml\" style=\"float: left; width: 45%;\"/>\n",
    "<img src=\"https://github.com/ML-course/master/blob/master/images/10_vgg16_fine_tuning2.png?raw=1\" alt=\"ml\" style=\"float: left; width: 45%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVKzdUhAYid3"
   },
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXXDuzcxYid3"
   },
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqheWKluYid3"
   },
   "source": [
    "* Load trained network, finetune\n",
    "    - Use a small learning rate, large number of epochs\n",
    "    - You don't want to unlearn too much: _catastrophic forgetting_\n",
    "    \n",
    "``` python\n",
    "model = load_model(os.path.join(model_dir, 'cats_and_dogs_small_3b.h5'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=optimizers.RMSprop(lr=1e-5),\n",
    "              metrics=['acc'])\n",
    "history = model.fit(\n",
    "      train_generator, steps_per_epoch=100, epochs=100,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRO7-hPqYid3"
   },
   "source": [
    "##### Takes a long time, uncomment if you really want to run it\n",
    "``` python\n",
    "from keras.models import load_model\n",
    "model = load_model(os.path.join(model_dir, 'cats_and_dogs_small_3b.h5'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-5),\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=10, # Repeat with epochs=100\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)\n",
    "model.save(os.path.join(model_dir, 'cats_and_dogs_small_4.h5'))\n",
    "with open(os.path.join(model_dir, 'cats_and_dogs_small_4_history.p'), 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKNFQRCWYid3"
   },
   "source": [
    "Almost 95% accuracy. The curves are quite noisy, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YcFFPK2VYid3"
   },
   "outputs": [],
   "source": [
    "history = pickle.load(open(\"../data/models/cats_and_dogs_small_3c_history.p\", \"rb\"))\n",
    "print(\"Max val_acc\",np.max(history['val_acc']))\n",
    "pd.DataFrame(history).plot(lw=2,style=['b:','r:','b-','r-']);\n",
    "plt.xlabel('epochs');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tUsaM6fYid3"
   },
   "source": [
    "* We can smooth the learning curves using a running average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDs_7R_VYid3"
   },
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.8):\n",
    "  smoothed_points = []\n",
    "  for point in points:\n",
    "    if smoothed_points:\n",
    "      previous = smoothed_points[-1]\n",
    "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "    else:\n",
    "      smoothed_points.append(point)\n",
    "  return smoothed_points\n",
    "\n",
    "smooth_history = {}\n",
    "smooth_history['loss'] = smooth_curve(history['loss'])\n",
    "smooth_history['acc'] = smooth_curve(history['acc'])\n",
    "smooth_history['val_loss'] = smooth_curve(history['val_loss'])\n",
    "smooth_history['val_acc'] = smooth_curve(history['val_acc'])\n",
    "\n",
    "print(\"Max val_acc\",np.max(smooth_history['val_acc']))\n",
    "pd.DataFrame(smooth_history).plot(lw=2,style=['b:','r:','b-','r-']);\n",
    "plt.xlabel('epochs');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQFZST0wYid4"
   },
   "source": [
    "Finally, evaluate the trained model on the test set. This is consistent with the validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTGMwCsAYid4"
   },
   "outputs": [],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')\n",
    "\n",
    "model = load_model(os.path.join(model_dir, 'cats_and_dogs_small_3c.h5'))\n",
    "test_loss, test_acc = model.evaluate(test_generator, steps=50)\n",
    "print('test acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-ZNfBiiYid4"
   },
   "source": [
    "## Take-aways\n",
    "* Convnets are ideal for attacking visual-classification problems.\n",
    "* They learn a hierarchy of modular patterns and concepts to represent the visual world.\n",
    "* Representations are easy to inspect\n",
    "* Data augmentation helps fight overfitting\n",
    "* You can use a pretrained convnet to build better models via transfer learning"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [
    "TYOcaGH5Yidv",
    "kaO1xfOGYidv",
    "qU2P04M8Yidw",
    "9zsULjBQYidw",
    "vac7H3HNYidw",
    "Z_FWWcutYidw",
    "aYvkxOPAYidw",
    "fMLBW1wLYidx",
    "MWkWdA1ZYidx",
    "LG9VevMHYidx",
    "AxG3qHwZYidy",
    "ztXKZpfXYidy",
    "-jt2dfLfYidy",
    "qaFft7RyYidy",
    "V1r71cd1Yidz",
    "Dt2x7EodYid0",
    "34QNNpdxYid0",
    "Co-4wIgXYid1",
    "5bs5a5voYid2",
    "irN1lLgHYid2",
    "2bWNYRaoYid2",
    "dkhlyKiJYid2",
    "hFTInQ94Yid3",
    "x-ZNfBiiYid4"
   ],
   "name": "Copy of 09 - Convolutional Neural Networks.ipynb",
   "provenance": []
  },
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "rise": {
   "theme": "white",
   "transition": "zoom"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
