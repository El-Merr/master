{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>html, body{overflow-y: visible !important} .CodeMirror{min-width:100% !important;} .rise-enabled .CodeMirror, .rise-enabled .output_subarea{font-size:100%; line-height:1.0; overflow: visible;} .output_subarea pre{width:100%}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preamble import *\n",
    "HTML('''<style>html, body{overflow-y: visible !important} .CodeMirror{min-width:100% !important;} .rise-enabled .CodeMirror, .rise-enabled .output_subarea{font-size:100%; line-height:1.0; overflow: visible;} .output_subarea pre{width:100%}</style>''') # For slides\n",
    "#HTML('''<style>html, body{overflow-y: visible !important} .output_subarea{font-size:100%; line-height:1.0; overflow: visible;}</style>''') # For slides\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Artificial Neuron\n",
    "![Neuron](images/neuron.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "### Can be stacked\n",
    "![Single layer mlp](images/single-layer-mlp.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "### Multiple layers of stacked neurons\n",
    "![Multiplayer perceptron](images/mlp.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivation\n",
    "    - Compositional features\n",
    "    \n",
    "![XOR visualization](images/xor-visualization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![XOR visualization](images/xor-visualization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![XOR visualization](images/xor-visualization3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Training single Neuron](images/training-single-neuron2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "![Single neuron nonlinear](images/single-neuron-nonlinear-features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multilayer Perceptron (MLP) \n",
    "![Image of a Neural Network](images/mlp.png) \n",
    "\n",
    "- Directed acyclic graph\n",
    "- Nodes are artificial neurons\n",
    "- Edges are connections between them \n",
    "* Feedforward Neural Network\n",
    "    - Neurons are ogranized in layers\n",
    "    - No connection between neurons within a layer\n",
    "    - All neurons in the same layer of the same type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multilayer Perceptron (MLP) \n",
    "![Image of a Neural Network](images/mlp.png)\n",
    "\n",
    "* Each layer creates a new representation of the input data:\n",
    "* $h^{(0)} = f^{(0)}(\\mathbf{x})$\n",
    "* $h^{(1)} = f^{(1)}(\\mathbf{h^{(0)}})$\n",
    "* $y = f^{(2)}(\\mathbf{h^{(1)}})$\n",
    "\n",
    "\n",
    "* Overall MLP is a function $f$\n",
    "* $y=f(x,\\theta)$\n",
    "\n",
    "* Nested functions: $f^{(3)}(f^{(2)}(f^{(1)}(x))))$\n",
    "    * First layer: $f^{(1)}$\n",
    "    * Second layer: $f^{(2)}$\n",
    "    * Third layer: $f^{(3)}$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "MLP\n",
    "===\n",
    "* Linear models have limitations capturing all realtionships between the features $x$\n",
    "* Non-linear models are difficult to train (non-convex) optimization\n",
    "    * local minimia, no guarantees of convergence\n",
    "* Linear models of non-linear transformation of $x$; $\\phi(x)$\n",
    "    * Choosing $\\phi(x)$ is the challenge\n",
    "    * If $\\phi$ is general transforation RBF; then model generalization is suffering\n",
    "    * Problem specific $\\phi$ requires siginificant human effort and does not scale well\n",
    "* The strategy of Deep Learning is to learn $\\phi$\n",
    "* $y=f(x;\\theta,w)=\\phi(x,\\theta)^\\top w$\n",
    "    * No convexity\n",
    "* MLP -> Deterministic mapping from x to y without recurrence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### MLP Capacity\n",
    "\n",
    "\n",
    "Multiple Neurons can carve many regions\n",
    "    - approximate arbitrary functions\n",
    "    - Universal approaximation theorem\n",
    "        - However, the number of neurons needed is binomial\n",
    "        - Upper bound on the number of regions we can separate with multi layer network grows exponentially\n",
    "\n",
    "## Number of layers (Depth) \n",
    "\n",
    "### Single hidden layer\n",
    "- Universal approximation theorem (Hornik, 1991)\n",
    "    - \"a single hidden layer neural network with a linear output unit can approximate any continuous function arbitrarily well, given enough units\"\n",
    "- Scales poorly\n",
    "    - To learn a complex function the model needs exponentially many neurons\n",
    "    - $\\sum_{j=0}^{n_1}{\\dbinom{n_0}{k}}$\n",
    "- However, since both the shallow network and the deep network can learn the same functions, we need to analyze the value of the depth in another way\n",
    "\n",
    "- Basically, they split the input space in piecewise linear units. It seems that deep neural networks have more segments (with the same number of neurons) which allows them to produce a more complex function approximate. Essentially, after partitioning the original input space piecewise linearly, each subsequent layer recognizes pieces of the original input such that the composition of these layers correspondingly identifies an exponential number of input regions. This is caused by the deep hierarchy which allows to apply the same computation across different regions of the input space.\n",
    "\n",
    "![folding the input space](images/folding-space.png)\n",
    "\n",
    "- The number of piecewise linear segments the input space can be split into grows exponentially with the number of layers of a deep neural network, whereas the growth is only polynomial with the number of neurons. This explains why deep neural networks perform so much better than shallow neural networks. \n",
    "\n",
    "- Finally, for shallow very wide network, no known algorithm that can learn on any type of function Learning algorithm that can optimize those parameters\n",
    "\n",
    "\n",
    "Architecture\n",
    "    - Input\n",
    "    - Hidden layers\n",
    "    - Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Can solve XOR\n",
    "\n",
    "![MLP XOR Start](images/mlp-xor-start.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent\n",
    "- Model: \n",
    "    - $o_\\mathbf{\\theta} = \\phi_1(\\mathbf{w_1}^\\top \\phi_2( \\mathbf{w_2}^\\top\\phi(\\mathbf{w_3}^\\top x)))$\n",
    "    - $\\theta : \\{\\mathbf{W}\\}$\n",
    "- Loss function: \n",
    "    - $L(\\mathbf{x}, y; \\mathbf{W}) = \\frac{1}{2n}\\sum_{i=0}^{n}{(o_\\theta - y)^2}$ \n",
    "    \n",
    "- Gradient of $L$ wrt $\\mathbf{W}$:\n",
    "    \n",
    "    - $\\frac{\\partial}{\\partial W}{L(.)}$ \n",
    "\n",
    "- biases omitted for simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Layered representation\n",
    "\n",
    "![MLP consolidated](images/mlp-consolidated.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "![MLP consolidated](images/mlp-consolidated2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Computation Graph\n",
    "- Vectorized form\n",
    "![MLP Compute Graph](images/mlp-compute-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Backprop Node](images/backprop-node2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Backprop Node](images/backprop-node-jacobian.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$J = {\\partial(\\mathbf{F}) \\over \\partial(\\mathbf{W})} =  \n",
    "\\left\\vert\\matrix{{\\partial f_1 \\over \\partial w_1} & {\\partial f_1 \\over \\partial w_2} & {\\partial f_1 \\over \\partial w_3} \\cr \n",
    "{\\partial f_2 \\over \\partial w_1} & {\\partial f_2\\over \\partial w_2} & {\\partial f_2 \\over \\partial w_3} \\cr \n",
    "{\\partial f_3 \\over \\partial w_1} & {\\partial f_3 \\over \\partial w_2} & {\\partial f_3 \\over \\partial w_3}}\\right\\vert $$\n",
    "- Activation of neuron n:\n",
    "    - $f_n$\n",
    "- Parameters of neuron n:\n",
    "    - $w_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$J = {\\partial(\\mathbf{F}) \\over \\partial(\\mathbf{W})} =  \n",
    "\\left\\vert\\matrix{{\\partial f_1 \\over \\partial w_1} & 0 & 0 \\cr \n",
    "0 & {\\partial f_2\\over \\partial w_2} & 0 \\cr \n",
    "0 & 0 & {\\partial f_3 \\over \\partial w_3}}\\right\\vert $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Mlp backprop compute graph](images/mlp-backprop-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "![MLP XOR Start](images/mlp-xor-start.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "![MLP XOR Start](images/mlp-xor-end.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "![MLP XOR Start](images/mlp-xor-end.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "(Show the examples with 3-4 neurons and xor)\n",
    "\n",
    "(Show examples with two layers tand 3 neurons)\n",
    "\n",
    "(Discuss the capacity (single layer vs. deep networks))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
